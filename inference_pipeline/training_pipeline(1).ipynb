{
  "cells": [
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mFVN2wBdjI2V",
        "outputId": "8e0ddaab-6279-4ae0-9e07-168c40a00461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n",
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting hopsworks\n",
            "  Downloading hopsworks-3.4.3.tar.gz (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m861.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hsfs[python]<3.5.0,>=3.4.0 (from hopsworks)\n",
            "  Downloading hsfs-3.4.5.tar.gz (170 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.3/170.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hsml<3.5.0,>=3.4.0 (from hopsworks)\n",
            "  Downloading hsml-3.4.5.tar.gz (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyhumps==1.6.1 (from hopsworks)\n",
            "  Downloading pyhumps-1.6.1-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from hopsworks) (2.31.0)\n",
            "Collecting furl (from hopsworks)\n",
            "  Downloading furl-2.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Collecting boto3 (from hopsworks)\n",
            "  Downloading boto3-1.34.15-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyjks (from hopsworks)\n",
            "  Downloading pyjks-20.0.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mock (from hopsworks)\n",
            "  Downloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hopsworks) (4.66.1)\n",
            "Requirement already satisfied: pandas<2.1.0 in /usr/local/lib/python3.10/dist-packages (from hsfs[python]<3.5.0,>=3.4.0->hopsworks) (1.5.3)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from hsfs[python]<3.5.0,>=3.4.0->hopsworks) (1.23.5)\n",
            "Collecting avro==1.11.0 (from hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading avro-1.11.0.tar.gz (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.10/dist-packages (from hsfs[python]<3.5.0,>=3.4.0->hopsworks) (2.0.24)\n",
            "Collecting PyMySQL[rsa] (from hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading PyMySQL-1.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting great_expectations==0.14.13 (from hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading great_expectations-0.14.13-py3-none-any.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markupsafe<2.1.0 (from hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading MarkupSafe-2.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from hsfs[python]<3.5.0,>=3.4.0->hopsworks) (5.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from hsfs[python]<3.5.0,>=3.4.0->hopsworks) (2023.6.0)\n",
            "Collecting pyhopshive[thrift] (from hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading PyHopsHive-0.6.4.1.dev0.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyarrow>=10.0 in /usr/local/lib/python3.10/dist-packages (from hsfs[python]<3.5.0,>=3.4.0->hopsworks) (10.0.1)\n",
            "Collecting confluent-kafka<=2.1.1 (from hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading confluent_kafka-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastavro<=1.8.2,>=1.4.11 (from hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<5,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (4.2.2)\n",
            "Requirement already satisfied: Click>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (8.1.7)\n",
            "Collecting colorama>=0.4.3 (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: cryptography>=3.2 in /usr/local/lib/python3.10/dist-packages (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (41.0.7)\n",
            "Collecting dataclasses (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: importlib-metadata>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (7.0.1)\n",
            "Requirement already satisfied: Ipython>=7.16.3 in /usr/local/lib/python3.10/dist-packages (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (7.34.0)\n",
            "Collecting jinja2<3.1.0,>=2.10 (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.6/133.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpatch>=1.22 (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (4.19.2)\n",
            "Requirement already satisfied: mistune>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (0.8.4)\n",
            "Requirement already satisfied: nbformat>=5.0 in /usr/local/lib/python3.10/dist-packages (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (5.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (23.2)\n",
            "Collecting pyparsing<3,>=2.4 (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2021.3 in /usr/local/lib/python3.10/dist-packages (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (2023.3.post1)\n",
            "Collecting ruamel.yaml<0.17.18,>=0.16 (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading ruamel.yaml-0.17.17-py3-none-any.whl (109 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (1.11.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.10/dist-packages (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (4.5.0)\n",
            "Collecting urllib3<1.27,>=1.25.4 (from great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->hopsworks) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->hopsworks) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->hopsworks) (2023.11.17)\n",
            "Collecting botocore<1.35.0,>=1.34.15 (from boto3->hopsworks)\n",
            "  Downloading botocore-1.34.15-py3-none-any.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->hopsworks)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->hopsworks)\n",
            "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from furl->hopsworks) (1.16.0)\n",
            "Collecting orderedmultidict>=1.0.1 (from furl->hopsworks)\n",
            "  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
            "Collecting javaobj-py3 (from pyjks->hopsworks)\n",
            "  Downloading javaobj_py3-0.4.3-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from pyjks->hopsworks) (0.5.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from pyjks->hopsworks) (0.3.0)\n",
            "Collecting pycryptodomex (from pyjks->hopsworks)\n",
            "  Downloading pycryptodomex-3.19.1-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting twofish (from pyjks->hopsworks)\n",
            "  Downloading twofish-0.3.0.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyhopshive[thrift]->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (0.18.3)\n",
            "Collecting thrift>=0.10.0 (from pyhopshive[thrift]->hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading thrift-0.16.0.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (3.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<5,>=4.0.0->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (0.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<5,>=4.0.0->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (0.12.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.2->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (1.16.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.7.0->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (3.17.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from Ipython>=7.16.3->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (67.7.2)\n",
            "Collecting jedi>=0.16 (from Ipython>=7.16.3->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from Ipython>=7.16.3->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from Ipython>=7.16.3->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from Ipython>=7.16.3->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from Ipython>=7.16.3->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from Ipython>=7.16.3->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from Ipython>=7.16.3->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from Ipython>=7.16.3->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from Ipython>=7.16.3->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (4.9.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch>=1.22->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.5.1->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.5.1->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.5.1->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.5.1->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (0.16.2)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.0->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (2.19.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.0->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (5.7.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.2->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (2.21)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->Ipython>=7.16.3->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->Ipython>=7.16.3->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->Ipython>=7.16.3->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (0.2.12)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat>=5.0->great_expectations==0.14.13->hsfs[python]<3.5.0,>=3.4.0->hopsworks) (4.1.0)\n",
            "Building wheels for collected packages: hopsworks, avro, hsml, hsfs, twofish, thrift, pyhopshive\n",
            "  Building wheel for hopsworks (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hopsworks: filename=hopsworks-3.4.3-py3-none-any.whl size=83151 sha256=aa94fae29f7beb743c73900b160321326b8e6fff584a5c7f697b45b707d561b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/dc/43/a6675965caddfd8b1716b5553e51485b37d5d874a99cae2b7b\n",
            "  Building wheel for avro (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro: filename=avro-1.11.0-py2.py3-none-any.whl size=115908 sha256=136de19e189ce0a7a8226d2106b5dbdb1eed154b26f096a8cfc0936ffc1e1a3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4f/67/5b7500b01c31c347b0751940659f94dac2394501a20c06a0db\n",
            "  Building wheel for hsml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hsml: filename=hsml-3.4.5-py3-none-any.whl size=108147 sha256=c37785dc0e991f4ccf44e95b08d694591163043f4ed28e73709c280c68dbcfaa\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/46/1d/424ec4848f7a946da98f970c53abc4936aeba3c6bf00ef1923\n",
            "  Building wheel for hsfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hsfs: filename=hsfs-3.4.5-py3-none-any.whl size=233846 sha256=4dfdbcbc52ff58d6256a3029ff1bae07152dc9910086ab0a14c4134eeb14ca4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/3d/5f/1ff8eb9d6246381d01e2bfa87cf375df9fd52a0ad70375527d\n",
            "  Building wheel for twofish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twofish: filename=twofish-0.3.0-cp310-cp310-linux_x86_64.whl size=24202 sha256=5dfb5481902ad7f32b3a6e1ed15feda54cea2a0af0e0436dd67e30324dbe61ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/3c/27/c390be4f3e8a299d4b2836f8daa19697eb991eacbfabe25031\n",
            "  Building wheel for thrift (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thrift: filename=thrift-0.16.0-cp310-cp310-linux_x86_64.whl size=373871 sha256=93b3c4e1b48663f5cbdd3aec3e4c976c1ad974b6736052bc0179b8fc42fecc17\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/f8/d2/acfd995e8247eb0cad372fa6a640a5fcf279ab2ed7c5c4490e\n",
            "  Building wheel for pyhopshive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyhopshive: filename=PyHopsHive-0.6.4.1.dev0-py3-none-any.whl size=48564 sha256=f1d14178cd06bffe082aa279267ccfd08ababbf6c5f6e41f32a966aeb9bf9f95\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/a5/da/3326e27d433d47029d233e314b993fdedb42b381650ea9dac7\n",
            "Successfully built hopsworks avro hsml hsfs twofish thrift pyhopshive\n",
            "Installing collected packages: twofish, pyhumps, javaobj-py3, dataclasses, confluent-kafka, urllib3, thrift, ruamel.yaml, pyparsing, PyMySQL, pycryptodomex, orderedmultidict, mock, markupsafe, jsonpointer, jmespath, jedi, fastavro, colorama, avro, pyjks, jsonpatch, jinja2, furl, botocore, s3transfer, pyhopshive, boto3, hsml, great_expectations, hsfs, hopsworks\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 2.1.3\n",
            "    Uninstalling MarkupSafe-2.1.3:\n",
            "      Successfully uninstalled MarkupSafe-2.1.3\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.2\n",
            "    Uninstalling Jinja2-3.1.2:\n",
            "      Successfully uninstalled Jinja2-3.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "werkzeug 3.0.1 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyMySQL-1.1.0 avro-1.11.0 boto3-1.34.15 botocore-1.34.15 colorama-0.4.6 confluent-kafka-2.1.1 dataclasses-0.6 fastavro-1.8.2 furl-2.1.3 great_expectations-0.14.13 hopsworks-3.4.3 hsfs-3.4.5 hsml-3.4.5 javaobj-py3-0.4.3 jedi-0.19.1 jinja2-3.0.3 jmespath-1.0.1 jsonpatch-1.33 jsonpointer-2.4 markupsafe-2.0.1 mock-5.1.0 orderedmultidict-1.0.1 pycryptodomex-3.19.1 pyhopshive-0.6.4.1.dev0 pyhumps-1.6.1 pyjks-20.0.0 pyparsing-2.4.7 ruamel.yaml-0.17.17 s3transfer-0.10.0 thrift-0.16.0 twofish-0.3.0 urllib3-1.26.18\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses",
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests --upgrade\n",
        "!pip install --upgrade\n",
        "!pip install hopsworks\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "id": "SFCmqojNezgt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer,BertModel\n",
        "import hopsworks"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 3,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwEuXf7i4LK0",
        "outputId": "712e8e34-4b85-4baa-9fa5-e368b8209f33"
      },
<<<<<<< HEAD
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
=======
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "HOPSWORKS_API_KEY = os.environ.get('HOPSWORKS_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
=======
      "execution_count": 4,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zujnHmJv_y7H",
        "outputId": "566fb58b-31b7-4b7b-9bef-9450d3a35c93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected. Call `.close()` to terminate connection gracefully.\n",
            "\n",
            "Multiple projects found. \n",
            "\n",
            "\t (1) jayeshv\n",
            "\t (2) id2223_enric\n",
            "\n",
            "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/197783\n",
            "Connected. Call `.close()` to terminate connection gracefully.\n"
          ]
        }
      ],
      "source": [
        "project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
        "fs = project.get_feature_store()\n",
        "hackernews_fg = fs.get_feature_group(name=\"hackernews_fg\", version=2)\n",
        "query = hackernews_fg.select_all()\n",
        "feature_view = fs.get_or_create_feature_view(name=\"hackernews_fv\",\n",
        "                                             version=2,\n",
        "                                             query=query)"
<<<<<<< HEAD
=======
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zujnHmJv_y7H",
        "outputId": "566fb58b-31b7-4b7b-9bef-9450d3a35c93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connection closed.\n",
            "Connected. Call `.close()` to terminate connection gracefully.\n",
            "\n",
            "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/197783\n",
            "Connected. Call `.close()` to terminate connection gracefully.\n"
          ]
        }
      ],
      "source": [
        "project = hopsworks.login(project=\"id2223_enric\")\n",
        "fs = project.get_feature_store()\n",
        "hackernews_fg = fs.get_feature_group(name=\"hackernews_fg\", version=2)\n",
        "query = hackernews_fg.select_all()\n",
        "feature_view = fs.get_or_create_feature_view(name=\"hackernews_fv\",\n",
        "                                             version=2,\n",
        "                                             query=query)\n",
        "\n"
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 27,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "cQpJmb6llVtt",
        "outputId": "3750bcd0-4045-4e76-ebd0-6eca86bfbc71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished: Reading data from Hopsworks, using ArrowFlight (4.29s) \n"
          ]
        },
        {
<<<<<<< HEAD
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VersionWarning: Incremented version to `68`.\n"
=======
          "ename": "FeatureStoreException",
          "evalue": "Could not read data using ArrowFlight. If the issue persists, use read_options={\"use_hive\": True} instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFlightUnavailableError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hsfs/core/arrow_flight_client.py\u001b[0m in \u001b[0;36mafs_error_handler_wrapper\u001b[0;34m(instance, *args, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hsfs/core/arrow_flight_client.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, query_object)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mdescriptor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlightDescriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hsfs/core/arrow_flight_client.py\u001b[0m in \u001b[0;36m_get_dataset\u001b[0;34m(self, descriptor)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescriptor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flight_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_to_ticket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/_flight.pyx\u001b[0m in \u001b[0;36mpyarrow._flight.FlightClient.get_flight_info\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/_flight.pyx\u001b[0m in \u001b[0;36mpyarrow._flight.check_flight_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFlightUnavailableError\u001b[0m: Flight returned unavailable error, with message: Connection reset by peer",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mFeatureStoreException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-6e6bac14df48>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hsfs/feature_view.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(self, test_size, train_start, train_end, test_start, test_end, description, extra_filter, statistics_config, read_options, spine)\u001b[0m\n\u001b[1;32m   1808\u001b[0m             \u001b[0mextra_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_filter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m         )\n\u001b[0;32m-> 1810\u001b[0;31m         td, df = self._feature_view_engine.get_training_data(\n\u001b[0m\u001b[1;32m   1811\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m             \u001b[0mread_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hsfs/core/feature_view_engine.py\u001b[0m in \u001b[0;36mget_training_data\u001b[0;34m(self, feature_view_obj, read_options, splits, training_dataset_obj, training_dataset_version, spine)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mspine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             )\n\u001b[0;32m--> 279\u001b[0;31m             split_df = engine.get_instance().get_training_data(\n\u001b[0m\u001b[1;32m    280\u001b[0m                 \u001b[0mtd_updated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_view_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hsfs/engine/python.py\u001b[0m in \u001b[0;36mget_training_data\u001b[0;34m(self, training_dataset_obj, feature_view_obj, query_obj, read_options)\u001b[0m\n\u001b[1;32m    544\u001b[0m     ):\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtraining_dataset_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             return self._prepare_transform_split_df(\n\u001b[0m\u001b[1;32m    547\u001b[0m                 \u001b[0mquery_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_dataset_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_view_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hsfs/engine/python.py\u001b[0m in \u001b[0;36m_prepare_transform_split_df\u001b[0;34m(self, query_obj, training_dataset_obj, feature_view_obj, read_option)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             result_dfs = self._random_split(\n\u001b[0;32m--> 596\u001b[0;31m                 \u001b[0mquery_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_option\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_dataset_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m             )\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hsfs/constructor/query.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, online, dataframe_type, read_options)\u001b[0m\n\u001b[1;32m    151\u001b[0m                 )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         return engine.get_instance().sql(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0msql_query\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_store_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hsfs/engine/python.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sql_query, feature_store, online_conn, dataframe_type, read_options, schema)\u001b[0m\n\u001b[1;32m    104\u001b[0m     ):\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0monline_conn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             return self._sql_offline(\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0msql_query\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mfeature_store\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hsfs/engine/python.py\u001b[0m in \u001b[0;36m_sql_offline\u001b[0;34m(self, sql_query, feature_store, dataframe_type, schema, hive_config)\u001b[0m\n\u001b[1;32m    130\u001b[0m     ):\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0marrow_flight_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_flyingduck_query_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             result_df = util.run_with_loading_animation(\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0;34m\"Reading data from Hopsworks, using ArrowFlight\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0marrow_flight_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_query\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hsfs/util.py\u001b[0m in \u001b[0;36mrun_with_loading_animation\u001b[0;34m(message, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hsfs/core/arrow_flight_client.py\u001b[0m in \u001b[0;36mafs_error_handler_wrapper\u001b[0;34m(instance, *args, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mFeatureStoreException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_message\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mafs_error_handler_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFeatureStoreException\u001b[0m: Could not read data using ArrowFlight. If the issue persists, use read_options={\"use_hive\": True} instead."
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
          ]
        }
      ],
      "source": [
<<<<<<< HEAD
        "X_train, X_test, Y_train, Y_test = feature_view.train_test_split(0.2)"
=======
        "X_train, X_test, y_train, y_test = feature_view.train_test_split(0.2)"
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "num_samples = 50\n",
        "\n",
        "def sample_from_bin(x, per_group_samples=10):\n",
        "    if len(x) <= per_group_samples:\n",
        "        return x\n",
        "    return x.sample(per_group_samples)\n",
        "\n",
        "def log_scores(x):\n",
        "    if x <=0 :\n",
        "        return x\n",
        "    return math.log(x)\n",
        "\n",
        "train_df = X_train.join(Y_train)\n",
        "test_df = X_test.join(Y_test)\n",
        "\n",
        "df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "df = df[df['score'] > 1]\n",
        "# df['score'] = df['score'].apply(log_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bins = np.linspace(df['score'].min(), df['score'].max(), num_samples + 1)\n",
        "df['score_bin'] = pd.cut(df['score'], bins=bins, labels=False, include_lowest=True)\n",
        "df_sampled = df.groupby('score_bin', group_keys=False).apply(sample_from_bin)\n",
        "df_sampled = df_sampled.drop(columns=['score_bin'])\n",
        "df = df_sampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 222 entries, 37552 to 125555\n",
            "Data columns (total 8 columns):\n",
            " #   Column       Non-Null Count  Dtype              \n",
            "---  ------       --------------  -----              \n",
            " 0   id           222 non-null    int32              \n",
            " 1   title        222 non-null    object             \n",
            " 2   url          222 non-null    object             \n",
            " 3   time         222 non-null    datetime64[ns, UTC]\n",
            " 4   descendants  222 non-null    int64              \n",
            " 5   by           222 non-null    object             \n",
            " 6   karma        222 non-null    int64              \n",
            " 7   score        222 non-null    float64            \n",
            "dtypes: datetime64[ns, UTC](1), float64(1), int32(1), int64(2), object(3)\n",
            "memory usage: 14.7+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
=======
      "execution_count": 7,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "id": "3BGpKXThmvL0"
      },
      "outputs": [],
      "source": [
<<<<<<< HEAD
        "X_train_titles = list(train_df['title'])"
=======
        "X_train_titles = list(X_train['title'])"
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 25,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCyXlFnbi24Y",
        "outputId": "0cf789b6-0c08-45d9-c2de-6afa629380a0"
      },
<<<<<<< HEAD
      "outputs": [],
=======
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n"
          ]
        }
      ],
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert = BertModel.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 9,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1alMZRjlnn9",
        "outputId": "ff34c5b6-d82b-49bc-c646-45cd44efd61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict size 30522\n"
          ]
        }
      ],
      "source": [
        "vocab = tokenizer.vocab\n",
        "print(\"dict size\", len(vocab))"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 10,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "id": "KqXJbFUUlsrt"
      },
      "outputs": [],
      "source": [
        "train_input = tokenizer(X_train_titles, max_length=30,\n",
        "                        add_special_tokens=True,\n",
        "                        truncation=True, padding=\"max_length\",\n",
        "                        return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test_titles =  list(test_df['title'])"
=======
      "execution_count": 11,
      "metadata": {
        "id": "c-YamQmInQ2F"
      },
      "outputs": [],
      "source": [
        "X_test_titles = list(X_test['title'])"
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 12,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "id": "01Q742R9nZrn"
      },
      "outputs": [],
      "source": [
        "test_input = tokenizer(X_test_titles, max_length=30,\n",
        "                       add_special_tokens=True,\n",
        "                       truncation=True, padding=\"max_length\",\n",
        "                       return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 13,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "id": "ezQnObLkfLf8"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, wrapped_input, scores):\n",
        "    self.wrapped_input = wrapped_input\n",
        "    self.scores = scores\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    input_dict = {}\n",
        "    for k in self.wrapped_input.keys():\n",
        "      input_dict[k] = self.wrapped_input[k][idx]\n",
        "    return input_dict, self.scores[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.scores)"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 14,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "id": "0eUB64Wc3dHI"
      },
      "outputs": [],
      "source": [
<<<<<<< HEAD
        "y_train_int = [int(score) for score in list(train_df['score'])]\n",
        "y_test_int = [int(score) for score in list(test_df['score'])]"
=======
        "y_train_int = [int(score) for score in list(y_train['score'])]\n",
        "y_test_int = [int(score) for score in list(y_test['score'])]"
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 15,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "id": "9dYMAzd3ud-q"
      },
      "outputs": [],
      "source": [
        "trainset = MyDataset(train_input, y_train_int)\n",
        "testset = MyDataset(test_input, y_test_int)"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 16,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcLOrje8uvsH",
        "outputId": "038600ca-27f2-49fd-f325-a1cbc98e3eef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainset size: 202\n",
            "valset size: 8\n",
            "testset size: 12\n"
          ]
        }
      ],
      "source": [
        "# Split val from trainset\n",
        "val_size = int(trainset.__len__() * 0.04)\n",
        "trainset, valset = random_split(trainset, [trainset.__len__() - val_size, val_size])\n",
        "print(f'trainset size: {trainset.__len__()}')\n",
        "print(f'valset size: {valset.__len__()}')\n",
        "print(f'testset size: {testset.__len__()}')"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 17,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "id": "B_SZsxulfrzu"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 24,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "id": "JGaaEzyLgHh7"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class BERT_classifier(nn.Module):\n",
        "    def __init__(self, bertmodel, num_score):\n",
        "        super(BERT_classifier, self).__init__()\n",
        "        self.bertmodel = bertmodel\n",
        "        self.dropout = nn.Dropout(p=bertmodel.config.hidden_dropout_prob)\n",
        "        self.linear1 = nn.Linear(bertmodel.config.hidden_size, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(512, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.linear3 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, wrapped_input):\n",
        "        hidden = self.bertmodel(**wrapped_input)\n",
        "        logits = hidden.last_hidden_state[:,0,:]\n",
        "        output = self.relu1(self.bn1(self.linear1(logits)))\n",
        "        output = self.relu2(self.bn2(self.linear2(output)))\n",
        "        score = self.linear3(output)\n",
        "        return score"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 19,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "id": "xXnTddtRwGzA"
      },
      "outputs": [],
      "source": [
        "model = BERT_classifier(bert, 1)"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 20,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYYdNdloxLPB",
        "outputId": "4a0d72e3-2430-4250-d3f4-fb50932ef8dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.3592],\n",
              "        [ 0.3454],\n",
              "        [-0.2638],\n",
              "        [-0.4534],\n",
              "        [-0.0516],\n",
              "        [ 0.3617],\n",
              "        [-0.3655],\n",
              "        [-0.2047]], grad_fn=<AddmmBackward0>)"
            ]
          },
<<<<<<< HEAD
          "execution_count": 24,
=======
          "execution_count": 20,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Forward one batch to test model and dataloader\n",
        "batch_data, batch_score = next(iter(train_loader))\n",
        "batch_scores = model(batch_data)\n",
        "print(batch_score.shape)\n",
        "batch_scores"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 21,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC5klWzUzYgY",
        "outputId": "f75321e2-100c-4205-cb5a-665e518a3d66"
      },
<<<<<<< HEAD
      "outputs": [],
      "source": [
        "from nn_factory import nn_factory\n",
        "nn_obj = nn_factory(model, device, tokenizer)"
=======
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n"
          ]
        }
      ],
      "source": [
        "from nn_factory import nn_factory\n",
        "nn_obj = nn_factory(model, device, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gGiKuZC27pR",
        "outputId": "12bb47e1-856c-4b03-a63c-38640d0121c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'float'>\n"
          ]
        }
      ],
      "source": [
        "print(type(list(y_train['score'])[0]))"
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 23,
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aioKKeX1ZFV",
        "outputId": "daefade9-a01c-41f5-d99d-45429386c9c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
<<<<<<< HEAD
=======
          "text": [
            "[epoch 1]train on 107731 data......\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 13467/13467 [10:06<00:00, 22.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training set: average loss: inf, acc: 11195/107731(10.392%)\n",
            "validation on 4488 data......\n",
            "Val set:Average loss:inf, acc:1/4488(0.022%)\n",
            "elapse: 613.04s \n",
            "\n",
            "improve validataion loss, saving model...\n",
            "\n",
            "[epoch 2]train on 107731 data......\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
          "text": [
            "[epoch 1]train on 202 data......\n"
          ]
        },
        {
<<<<<<< HEAD
          "name": "stderr",
          "output_type": "stream",
=======
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training set: average loss: inf, acc: 11271/107731(10.462%)\n",
            "validation on 4488 data......\n",
            "Val set:Average loss:inf, acc:103/4488(2.295%)\n",
            "elapse: 611.14s \n",
            "\n",
            "improve validataion loss, saving model...\n",
            "\n",
            "[epoch 3]train on 107731 data......\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 13467/13467 [10:15<00:00, 21.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
          "text": [
            "  0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
<<<<<<< HEAD
=======
          "text": [
            "100%|██████████| 13467/13467 [10:13<00:00, 21.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
          "text": [
            "UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  0%|          | 0/26 [00:02<?, ?it/s]\n"
          ]
        },
        {
<<<<<<< HEAD
          "ename": "RuntimeError",
          "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m----> 5\u001b[0m \u001b[43mnn_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/id2223/project/inference_pipeline/nn_factory.py:24\u001b[0m, in \u001b[0;36mnn_factory.fit\u001b[0;34m(self, epoch, optimizer, train_loader, val_loader, model_save_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     23\u001b[0m     epoch_begin \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 24\u001b[0m     cur_train_loss, cur_train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     cur_val_loss, cur_val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval(val_loader)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapse: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124ms \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m epoch_begin))\n",
            "File \u001b[0;32m~/id2223/project/inference_pipeline/nn_factory.py:86\u001b[0m, in \u001b[0;36mnn_factory.train\u001b[0;34m(self, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     83\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     84\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 86\u001b[0m     mse_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmse_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     89\u001b[0m mse_loss \u001b[38;5;241m=\u001b[39m mse_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
=======
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 13467/13467 [10:15<00:00, 21.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training set: average loss: inf, acc: 11084/107731(10.289%)\n",
            "validation on 4488 data......\n",
            "Val set:Average loss:inf, acc:1/4488(0.022%)\n",
            "elapse: 621.17s \n",
            "\n",
            "improve validataion loss, saving model...\n",
            "\n"
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
<<<<<<< HEAD
        "nn_obj.fit(25, optimizer, train_loader, val_loader, './')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "state = {\n",
        "        'epochs': 30,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "torch.save(state, 'last_model.pt')"
=======
        "nn_obj.fit(5, optimizer, train_loader, val_loader, './')"
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
<<<<<<< HEAD
=======
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
>>>>>>> 450fe1620a58908ae9bbb2ea3dcc837c3601444f
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
