{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import hopsworks\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_classifier(nn.Module):\n",
    "    def __init__(self, bertmodel, num_score):\n",
    "        super(BERT_classifier, self).__init__()\n",
    "        self.bertmodel = bertmodel\n",
    "        self.dropout = nn.Dropout(p=bertmodel.config.hidden_dropout_prob)\n",
    "        self.linear = nn.Linear(bertmodel.config.hidden_size, num_score)\n",
    "\n",
    "    def forward(self, wrapped_input):\n",
    "        hidden = self.bertmodel(**wrapped_input)\n",
    "        _, pooler_output = hidden[0], hidden[1]\n",
    "        output_value = self.linear(pooler_output).squeeze()\n",
    "        score = torch.sigmoid(output_value) * 1000\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT_classifier(\n",
       "  (bertmodel): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = BERT_classifier(bert, 1)\n",
    "model.load_state_dict(torch.load('./model_1.pt', map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/197783\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login(project=\"id2223_enric\")\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using ArrowFlight (7.59s) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: Incremented version to `30`.\n"
     ]
    }
   ],
   "source": [
    "hackernews_fg = fs.get_feature_group(name=\"hackernews_fg\", version=2)\n",
    "query = hackernews_fg.select_all()\n",
    "feature_view = fs.get_or_create_feature_view(name=\"hackernews_fv\",\n",
    "                                             version=2,\n",
    "                                             query=query)\n",
    "X_train, X_test, y_train, y_test = feature_view.train_test_split(0.2)\n",
    "X_train_titles = list(X_train['title'])\n",
    "y_train_int = [int(score) for score in list(y_train['score'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fc3ad06ea840dc851e6d02235c97ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_1.pt:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ID2223/hackernews_upvotes_predictor_model/commit/9f4498b471d3fe8790d81f6bc9bcd460bd1fafcf', commit_message='Upload model_1.pt with huggingface_hub', commit_description='', oid='9f4498b471d3fe8790d81f6bc9bcd460bd1fafcf', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj='./model_1.pt',\n",
    "    path_in_repo='model_1.pt',\n",
    "    repo_id='ID2223/hackernews_upvotes_predictor_model',\n",
    "    repo_type='model',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45a3318014845cbbc04cc100431f2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/197783/dataset/upload/%2FProjects%2Fid2223_enric%2FModels%2Fhackernews-model%2F1). Server response: \nHTTP code: 500, HTTP reason: Internal Server Error, body: b'{\"errorCode\":120000,\"devMsg\":\"Singleton StagingManager is unavailable because its original initialization failed.\",\"errorMsg\":\"A generic error occurred.\"}', error code: 120000, error msg: A generic error occurred., user msg: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m hackernews_model \u001b[38;5;241m=\u001b[39m mr\u001b[38;5;241m.\u001b[39mpython\u001b[38;5;241m.\u001b[39mcreate_model(\n\u001b[1;32m      2\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhackernews-model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     model_schema\u001b[38;5;241m=\u001b[39mmodel_schema,\n\u001b[1;32m      4\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHackernews-upvotes-predictor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Upload the model to the model registry, including all files in 'model_dir'\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m hackernews_model\u001b[38;5;241m.\u001b[39msave(model_dir)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsml/model.py:96\u001b[0m, in \u001b[0;36mModel.save\u001b[0;34m(self, model_path, await_registration, keep_original_files)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path, await_registration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m480\u001b[39m, keep_original_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Persist this model including model files and metadata to the model registry.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    # Arguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m        `Model`: The model metadata object.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_engine\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     98\u001b[0m         model_path,\n\u001b[1;32m     99\u001b[0m         await_registration\u001b[38;5;241m=\u001b[39mawait_registration,\n\u001b[1;32m    100\u001b[0m         keep_original_files\u001b[38;5;241m=\u001b[39mkeep_original_files,\n\u001b[1;32m    101\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsml/engine/model_engine.py:333\u001b[0m, in \u001b[0;36mModelEngine.save\u001b[0;34m(self, model_instance, model_path, await_registration, keep_original_files)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m be:\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_api\u001b[38;5;241m.\u001b[39mrm(model_instance\u001b[38;5;241m.\u001b[39mversion_path)\n\u001b[0;32m--> 333\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m be\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel created, explore it at \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_instance\u001b[38;5;241m.\u001b[39mget_url())\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_instance\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsml/engine/model_engine.py:297\u001b[0m, in \u001b[0;36mModelEngine.save\u001b[0;34m(self, model_instance, model_path, await_registration, keep_original_files)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# check local relative\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m    295\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), model_path)\n\u001b[1;32m    296\u001b[0m ):  \u001b[38;5;66;03m# check local relative\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_model_from_local_or_hopsfs_mount(\n\u001b[1;32m    298\u001b[0m         model_instance\u001b[38;5;241m=\u001b[39mmodel_instance,\n\u001b[1;32m    299\u001b[0m         model_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), model_path),\n\u001b[1;32m    300\u001b[0m         keep_original_files\u001b[38;5;241m=\u001b[39mkeep_original_files,\n\u001b[1;32m    301\u001b[0m         update_upload_progress\u001b[38;5;241m=\u001b[39mupdate_upload_progress,\n\u001b[1;32m    302\u001b[0m     )\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# check project relative\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_api\u001b[38;5;241m.\u001b[39mpath_exists(\n\u001b[1;32m    305\u001b[0m     model_path\n\u001b[1;32m    306\u001b[0m ):  \u001b[38;5;66;03m# check hdfs relative and absolute\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsml/engine/model_engine.py:161\u001b[0m, in \u001b[0;36mModelEngine._save_model_from_local_or_hopsfs_mount\u001b[0;34m(self, model_instance, model_path, keep_original_files, update_upload_progress)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copy_or_move_hopsfs_model(\n\u001b[1;32m    153\u001b[0m         from_hdfs_model_path\u001b[38;5;241m=\u001b[39mmodel_path\u001b[38;5;241m.\u001b[39mreplace(\n\u001b[1;32m    154\u001b[0m             constants\u001b[38;5;241m.\u001b[39mMODEL_REGISTRY\u001b[38;5;241m.\u001b[39mHOPSFS_MOUNT_PREFIX, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m         update_upload_progress\u001b[38;5;241m=\u001b[39mupdate_upload_progress,\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upload_local_model(\n\u001b[1;32m    162\u001b[0m         from_local_model_path\u001b[38;5;241m=\u001b[39mmodel_path,\n\u001b[1;32m    163\u001b[0m         to_model_version_path\u001b[38;5;241m=\u001b[39mmodel_instance\u001b[38;5;241m.\u001b[39mversion_path,\n\u001b[1;32m    164\u001b[0m         update_upload_progress\u001b[38;5;241m=\u001b[39mupdate_upload_progress,\n\u001b[1;32m    165\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsml/engine/model_engine.py:142\u001b[0m, in \u001b[0;36mModelEngine._upload_local_model\u001b[0;34m(self, from_local_model_path, to_model_version_path, update_upload_progress)\u001b[0m\n\u001b[1;32m    140\u001b[0m     update_upload_progress(n_dirs, n_files)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f_name \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mupload(root \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m f_name, remote_base_path)\n\u001b[1;32m    143\u001b[0m     n_files \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    144\u001b[0m     update_upload_progress(n_dirs, n_files)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsml/engine/local_engine.py:38\u001b[0m, in \u001b[0;36mLocalEngine.upload\u001b[0;34m(self, local_path, remote_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m local_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_abs_path(local_path)\n\u001b[1;32m     37\u001b[0m remote_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepend_project_path(remote_path)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_api\u001b[38;5;241m.\u001b[39mupload(local_path, remote_path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsml/core/dataset_api.py:60\u001b[0m, in \u001b[0;36mDatasetApi.upload\u001b[0;34m(self, local_abs_path, upload_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m query_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflowCurrentChunkSize\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m     58\u001b[0m query_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflowChunkNumber\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m chunk_number\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upload_request(query_params, upload_path, file_name, chunk)\n\u001b[1;32m     62\u001b[0m chunk_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsml/core/dataset_api.py:80\u001b[0m, in \u001b[0;36mDatasetApi._upload_request\u001b[0;34m(self, params, path, file_name, chunk)\u001b[0m\n\u001b[1;32m     77\u001b[0m path_params \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\"\u001b[39m, _client\u001b[38;5;241m.\u001b[39m_project_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupload\u001b[39m\u001b[38;5;124m\"\u001b[39m, path]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Flow configuration params are sent as form data\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m _client\u001b[38;5;241m.\u001b[39m_send_request(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, path_params, data\u001b[38;5;241m=\u001b[39mparams, files\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m: (file_name, chunk)}\n\u001b[1;32m     82\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsml/decorators.py:35\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(inst, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsml/client/base.py:108\u001b[0m, in \u001b[0;36mClient._send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files)\u001b[0m\n\u001b[1;32m    105\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39msend(prepped, verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mRestAPIError\u001b[0m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/197783/dataset/upload/%2FProjects%2Fid2223_enric%2FModels%2Fhackernews-model%2F1). Server response: \nHTTP code: 500, HTTP reason: Internal Server Error, body: b'{\"errorCode\":120000,\"devMsg\":\"Singleton StagingManager is unavailable because its original initialization failed.\",\"errorMsg\":\"A generic error occurred.\"}', error code: 120000, error msg: A generic error occurred., user msg: "
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
