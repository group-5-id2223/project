{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"7dc0d9be1d31ece641148d1afb09bd03dd6651e1"},"source":["# Hacker News Submission Score Predictor w/ Keras and TensorFlow\n","\n","by Max Woolf ([@minimaxir](https://minimaxir.com))"]},{"cell_type":"code","execution_count":1,"metadata":{"_uuid":"93ba18402e731c4293323a0aeb307d1109b53b0b","collapsed":true,"scrolled":true,"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import hopsworks\n","import torch\n","import torchtext\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":39,"metadata":{"_uuid":"3500e7029ecde96fc6decfb602be4a5298641eae","collapsed":true,"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>url</th>\n","      <th>score</th>\n","      <th>time</th>\n","      <th>descendants</th>\n","      <th>by</th>\n","      <th>karma</th>\n","      <th>domain</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>8535571.0</td>\n","      <td>5 Guidelines for Recovery Drills Within the AW...</td>\n","      <td>http://www.n2ws.com/blog/5-guidelines-recovery...</td>\n","      <td>2.0</td>\n","      <td>1.414700e+09</td>\n","      <td>0.0</td>\n","      <td>iamondemand</td>\n","      <td>83.0</td>\n","      <td>www.n2ws.com</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>38122977.0</td>\n","      <td>Why do we allow ourselves to hold ungrounded a...</td>\n","      <td>https://meltingasphalt.com/crony-beliefs/</td>\n","      <td>3.0</td>\n","      <td>1.698974e+09</td>\n","      <td>0.0</td>\n","      <td>TheIronYuppie</td>\n","      <td>1547.0</td>\n","      <td>meltingasphalt.com</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>7349975.0</td>\n","      <td>Japan Said to Be Ready to Impose Bitcoin Rules</td>\n","      <td>http://dealbook.nytimes.com/2014/03/05/japan-s...</td>\n","      <td>2.0</td>\n","      <td>1.394054e+09</td>\n","      <td>0.0</td>\n","      <td>JumpCrisscross</td>\n","      <td>129974.0</td>\n","      <td>dealbook.nytimes.com</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>19972463.0</td>\n","      <td>“Maslow’s pyramid” is based on an elitist misr...</td>\n","      <td>https://qz.com/work/1588491/maslow-didnt-make-...</td>\n","      <td>2.0</td>\n","      <td>1.558460e+09</td>\n","      <td>0.0</td>\n","      <td>wjSgoWPm5bWAhXB</td>\n","      <td>2456.0</td>\n","      <td>qz.com</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>21521793.0</td>\n","      <td>The Girl Who Never Came Back (1960)</td>\n","      <td>https://www.americanheritage.com/girl-who-neve...</td>\n","      <td>2.0</td>\n","      <td>1.573630e+09</td>\n","      <td>0.0</td>\n","      <td>smacktoward</td>\n","      <td>54886.0</td>\n","      <td>www.americanheritage.com</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>3471206.0</td>\n","      <td>Pasadena Cheeseburger Week - We Went There</td>\n","      <td>http://aloneinaforest.com/cheeseburger-in-para...</td>\n","      <td>1.0</td>\n","      <td>1.326731e+09</td>\n","      <td>-1.0</td>\n","      <td>darlingalice</td>\n","      <td>2.0</td>\n","      <td>aloneinaforest.com</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>3346871.0</td>\n","      <td>Recreating the original Macintosh boot beep in...</td>\n","      <td>http://romulusetrem.us/bootbeep/</td>\n","      <td>2.0</td>\n","      <td>1.323764e+09</td>\n","      <td>0.0</td>\n","      <td>pom</td>\n","      <td>73.0</td>\n","      <td>romulusetrem.us</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>17079306.0</td>\n","      <td>Google One is coming soon</td>\n","      <td>https://one.google.com</td>\n","      <td>81.0</td>\n","      <td>1.526434e+09</td>\n","      <td>86.0</td>\n","      <td>tvvocold</td>\n","      <td>1946.0</td>\n","      <td>one.google.com</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>4388709.0</td>\n","      <td>Let's Build a Tesla Museum</td>\n","      <td>http://theoatmeal.com/blog/tesla_museum</td>\n","      <td>1.0</td>\n","      <td>1.345071e+09</td>\n","      <td>-1.0</td>\n","      <td>cjdavis</td>\n","      <td>174.0</td>\n","      <td>theoatmeal.com</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>11422153.0</td>\n","      <td>Fair Source licensing is the worst thing to ha...</td>\n","      <td>http://www.techrepublic.com/article/fair-sourc...</td>\n","      <td>2.0</td>\n","      <td>1.459780e+09</td>\n","      <td>0.0</td>\n","      <td>alxsanchez</td>\n","      <td>450.0</td>\n","      <td>www.techrepublic.com</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           id                                              title  \\\n","0   8535571.0  5 Guidelines for Recovery Drills Within the AW...   \n","1  38122977.0  Why do we allow ourselves to hold ungrounded a...   \n","2   7349975.0     Japan Said to Be Ready to Impose Bitcoin Rules   \n","3  19972463.0  “Maslow’s pyramid” is based on an elitist misr...   \n","4  21521793.0                The Girl Who Never Came Back (1960)   \n","5   3471206.0         Pasadena Cheeseburger Week - We Went There   \n","6   3346871.0  Recreating the original Macintosh boot beep in...   \n","7  17079306.0                          Google One is coming soon   \n","8   4388709.0                         Let's Build a Tesla Museum   \n","9  11422153.0  Fair Source licensing is the worst thing to ha...   \n","\n","                                                 url  score          time  \\\n","0  http://www.n2ws.com/blog/5-guidelines-recovery...    2.0  1.414700e+09   \n","1          https://meltingasphalt.com/crony-beliefs/    3.0  1.698974e+09   \n","2  http://dealbook.nytimes.com/2014/03/05/japan-s...    2.0  1.394054e+09   \n","3  https://qz.com/work/1588491/maslow-didnt-make-...    2.0  1.558460e+09   \n","4  https://www.americanheritage.com/girl-who-neve...    2.0  1.573630e+09   \n","5  http://aloneinaforest.com/cheeseburger-in-para...    1.0  1.326731e+09   \n","6                   http://romulusetrem.us/bootbeep/    2.0  1.323764e+09   \n","7                             https://one.google.com   81.0  1.526434e+09   \n","8            http://theoatmeal.com/blog/tesla_museum    1.0  1.345071e+09   \n","9  http://www.techrepublic.com/article/fair-sourc...    2.0  1.459780e+09   \n","\n","   descendants               by     karma                    domain  \n","0          0.0      iamondemand      83.0              www.n2ws.com  \n","1          0.0    TheIronYuppie    1547.0        meltingasphalt.com  \n","2          0.0   JumpCrisscross  129974.0      dealbook.nytimes.com  \n","3          0.0  wjSgoWPm5bWAhXB    2456.0                    qz.com  \n","4          0.0      smacktoward   54886.0  www.americanheritage.com  \n","5         -1.0     darlingalice       2.0        aloneinaforest.com  \n","6          0.0              pom      73.0           romulusetrem.us  \n","7         86.0         tvvocold    1946.0            one.google.com  \n","8         -1.0          cjdavis     174.0            theoatmeal.com  \n","9          0.0       alxsanchez     450.0      www.techrepublic.com  "]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["# project = hopsworks.login(project='id2223_enric')\n","# fs = project.get_feature_store()\n","# hackernews_fg = fs.get_feature_group(\"hackernews_fg\", 2)\n","# query = hackernews_fg.select_all()\n","# feature_view = fs.get_or_create_feature_view(name=\"hackernews_fv\",\n","#                                   version=2,\n","#                                   description=\"Hackernews feature view\",\n","#                                   labels=[\"score\"],\n","#                                   query=query)\n","from sklearn.model_selection import train_test_split\n","from urllib.parse import urlparse\n","\n","def url_to_domain(url):\n","    parsed_url = urlparse(url)\n","\n","    domain = parsed_url.netloc\n","    return domain\n","\n","feature_view = pd.read_csv('../data/pd_combined.csv')\n","\n","train_df, test_df = train_test_split(feature_view, test_size=0.05)\n","\n","df = train_df.sample(frac=1, random_state=123).dropna().reset_index(drop=True)\n","df['domain'] = df['url'].apply(url_to_domain)\n","\n","df.head(10)"]},{"cell_type":"code","execution_count":40,"metadata":{"_uuid":"644a5e101b25edf934fe33b672ec633c1e398336","collapsed":true,"trusted":true},"outputs":[],"source":["from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import Vocab\n","\n","def pad_sequences(seq, max_len=15):\n","    seq =  torch.tensor(seq)\n","    seq = nn.ConstantPad1d((0, max_len - len(seq)), 0)(seq)\n","    return seq\n","\n","tokenizer = get_tokenizer(\"basic_english\")"]},{"cell_type":"code","execution_count":41,"metadata":{"_uuid":"b55daa311c443e0cf961a9d424b03e32baa10b9a","collapsed":true,"trusted":true},"outputs":[],"source":["from collections import Counter\n","from functools import partial\n","\n","counter = Counter()\n","for title in df['title']:\n","    counter.update(tokenizer(title))\n","vocab = Vocab(counter)\n","text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n","df['title'] = df['title'].apply(text_pipeline)\n","df['title'] = df['title'].apply(pad_sequences)"]},{"cell_type":"markdown","metadata":{"_uuid":"a924d7ba85067fa07cc35eba48c2cfe268bbcabb"},"source":["### Top Domains\n","\n","Identify the top *n* domains by count (in this case *n* = 100), then transform it to a *n*D vector for each post."]},{"cell_type":"code","execution_count":42,"metadata":{"_uuid":"bba9a0d62b8a75da61bdc0bb521507107ef5516d","collapsed":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["domain\n","github.com                4773\n","medium.com                3468\n","www.youtube.com           2783\n","www.nytimes.com           2114\n","techcrunch.com            1542\n","                          ... \n","bit.ly                     104\n","apnews.com                 103\n","blogs.wsj.com              103\n","www.usatoday.com           103\n","bits.blogs.nytimes.com     102\n","Name: count, Length: 100, dtype: int64\n"]}],"source":["num_domains = 100\n","\n","domain_counts = df['domain'].value_counts()[0:num_domains]\n","\n","print(domain_counts)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["titles = torch.stack([title for title in df['title'].values])"]},{"cell_type":"code","execution_count":47,"metadata":{"_uuid":"e8a2182d48e0af02c1db7f449ac0f8e8a7bccaad","collapsed":true,"trusted":true},"outputs":[{"data":{"text/plain":["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import LabelBinarizer\n","\n","top_domains = np.array(domain_counts.index, dtype=object)\n","\n","domain_encoder = LabelBinarizer()\n","domain_encoder.fit(top_domains)\n","\n","domains = domain_encoder.transform(df['domain'].values.astype(str))\n","domains[0]"]},{"cell_type":"markdown","metadata":{"_uuid":"892f13419e923efb918132ebd14990038753e6a0"},"source":["### Day-of-Week and Hour\n","\n","Convert day-of-week to a 7D vector and hours to a 24D vector. Both pandas and keras have useful functions for this workflow."]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["def to_categorical(y, num_classes):\n","    \"\"\" 1-hot encodes a tensor \"\"\"\n","    return np.eye(num_classes, dtype='uint8')[y]"]},{"cell_type":"code","execution_count":55,"metadata":{"_uuid":"af8e27fe36ec1e49817158de677b4070e42e3a06","collapsed":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0 0 0 1 0 0 0]\n"," [0 0 0 1 0 0 0]\n"," [0 0 0 1 0 0 0]\n"," [0 0 0 1 0 0 0]\n"," [0 0 0 1 0 0 0]]\n","[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"]}],"source":["# from keras.utils import to_categorical\n","\n","dayofweeks = to_categorical(pd.to_datetime(df['time']).dt.dayofweek, 7)\n","hours = to_categorical(pd.to_datetime(df['time']).dt.hour, 24)\n","\n","print(dayofweeks[0:5])\n","print(hours[0:5])"]},{"cell_type":"markdown","metadata":{"_uuid":"869b7d1dcc98e712eeef2b1e2b11acaee8effeb2"},"source":["## Sample Weights\n","\n","Weight `score=1` samples lower so model places a higher importance on atypical submissions."]},{"cell_type":"code","execution_count":56,"metadata":{"_uuid":"e2e59af44823777b2636f991fb4e361cbb574689","collapsed":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[1. 1. 1. 1. 1.]\n"]}],"source":["weights = np.where(df['score'].values == 1, 0.5, 1.0)\n","print(weights[0:5])"]},{"cell_type":"markdown","metadata":{"_uuid":"7ba7a108c4bf2b64bae171da853ab7ba3c512f58"},"source":["## Trend and Time on New\n","\n","Unused in final model, but kept here for reference."]},{"cell_type":"code","execution_count":57,"metadata":{"_uuid":"ff955f9b48377059ad8979ec5824afcc2c4661d4","collapsed":true,"trusted":true},"outputs":[{"data":{"text/plain":["array([[0.46796003],\n","       [0.99111293],\n","       [0.42996415],\n","       [0.73252349],\n","       [0.76044037]])"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","trend_encoder = MinMaxScaler()\n","trends = trend_encoder.fit_transform(pd.to_datetime(df['time']).values.reshape(-1, 1))\n","trends[0:5]"]},{"cell_type":"code","execution_count":58,"metadata":{"_uuid":"b2b95eb166c862d0cf3ac0c783b61dd9ceb4530b","collapsed":true,"trusted":true},"outputs":[],"source":["# newtime_encoder = MinMaxScaler()\n","# newtimes = trend_encoder.fit_transform(df['time_on_new'].values.reshape(-1, 1))\n","# newtimes[0:5]"]},{"cell_type":"markdown","metadata":{"_uuid":"4f6ab55f12a1db42e88840a5184c63317f409747"},"source":["## Build the Model Prototype"]},{"cell_type":"markdown","metadata":{"_uuid":"ec736e93b7a073e8bf68f29b80ca63dbebe3a004"},"source":["Add R^2 as a performance metric: https://jmlb.github.io/ml/2017/03/20/CoeffDetermination_CustomMetric4Keras/"]},{"cell_type":"code","execution_count":116,"metadata":{"_uuid":"6baf595ec8a40471438fcaada96b7e3f8ebf6c7a","collapsed":true,"trusted":true},"outputs":[],"source":["# from keras import backend as K\n","epsilon = 1e-7\n","def r_2(y_true, y_pred):\n","    SS_res =  torch.sum(torch.square( y_true - y_pred )) \n","    SS_tot = torch.sum(torch.square( y_true - torch.mean(y_true) ) ) \n","    return ( 1 - SS_res/(SS_tot + epsilon) )"]},{"cell_type":"markdown","metadata":{"_uuid":"ffbabccecde5331ab74518451cccc602b4dc4184"},"source":["Minimizing `mse` loss as typical for regression problems will not work, as the model will realize that selecting 1 unilaterally accomplishes this task the best.\n","\n","Instead, create a hybrid loss of `mae`, `msle`, and `poisson` (see Keras's docs for more info: https://github.com/keras-team/keras/blob/master/keras/losses.py) The latter two losses can account for very high values much better; perfect for the hyper-skewed data."]},{"cell_type":"code","execution_count":120,"metadata":{"_uuid":"bf8e5369a0124adc6d2b47748270608fc0a2bb85","collapsed":true,"trusted":true},"outputs":[],"source":["def hybrid_loss(y_true, y_pred):\n","    weight_mae = 0.1\n","    weight_msle = 1.\n","    weight_poisson = 0.1\n","    \n","    mae_loss = weight_mae * torch.mean(torch.abs(y_pred - y_true), axis=-1)\n","    \n","    first_log = torch.log(torch.clip(y_pred, 1, None) + 1.)\n","    second_log = torch.log(torch.clip(y_true, epsilon, None) + 1.)\n","    msle_loss = weight_msle * torch.mean(torch.square(first_log - second_log), axis=-1)\n","    \n","    poisson_loss = weight_poisson * torch.mean(y_pred - y_true * torch.log(y_pred + epsilon), axis=-1)\n","    return torch.mean(mae_loss + msle_loss + poisson_loss)"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]}],"source":["domains = torch.tensor(domains)\n","dayofweeks = torch.tensor(dayofweeks)\n","hours = torch.tensor(hours)"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"data":{"text/plain":["(torch.Size([129802, 15]),\n"," torch.Size([129802, 100]),\n"," torch.Size([129802, 7]),\n"," torch.Size([129802, 24]))"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["titles.shape, domains.shape, dayofweeks.shape, hours.shape"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, num_words=15, num_hidden_layers=5):\n","        super().__init__()\n","\n","        self.embedding_titles = nn.Embedding(31001, 50)\n","        self.spatial_dropout = nn.Dropout2d(0.2)\n","        self.rnn_titles = nn.LSTM(50, 128)\n","\n","        self.hidden_layers = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Linear(259, 259),\n","                nn.ReLU(),\n","                nn.BatchNorm1d(259),\n","                nn.Dropout(0.5)\n","            )\n","            for _ in range(num_hidden_layers)\n","        ])\n","\n","        self.output_layer = nn.Linear(259, 1)\n","\n","    def forward(self, input_titles, input_domains, input_dayofweeks, input_hours):\n","        embedding_titles = self.embedding_titles(input_titles)\n","        spatial_dropout = self.spatial_dropout(embedding_titles)\n","        rnn_titles, _ = self.rnn_titles(spatial_dropout.permute(1, 0, 2))\n","\n","        concat = torch.cat([rnn_titles[-1], input_domains, input_dayofweeks, input_hours], dim=1)\n","        i = 0\n","        for layer in self.hidden_layers:\n","            concat = layer(concat)\n","\n","        output = self.output_layer(concat)\n","        return output"]},{"cell_type":"code","execution_count":110,"metadata":{},"outputs":[],"source":["import torch.optim as optim\n","model = Model()\n","batch_lr = 1e-3\n","num_epochs = 100\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","lr_scheduler = optim.lr_scheduler.LinearLR(optimizer)"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/10817 [00:00<?, ?it/s]UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n","  0%|          | 4/10817 [00:00<04:33, 39.55it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:47<00:00, 37.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0/100 ; Training loss: 4.07519202375377\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:51<00:00, 37.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1/100 ; Training loss: 4.01695658219623\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:44<00:00, 37.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2/100 ; Training loss: 4.013249814462836\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:39<00:00, 38.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 3/100 ; Training loss: 4.011533654568707\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:31<00:00, 39.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 4/100 ; Training loss: 4.010807378475456\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:34<00:00, 39.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 5/100 ; Training loss: 4.010070092854905\n"]},{"name":"stderr","output_type":"stream","text":[" 34%|███▍      | 3723/10817 [01:33<02:59, 39.50it/s]"]}],"source":["from tqdm import tqdm\n","\n","batch_size = 12\n","total_len = titles.shape[0]\n","gt_scores = torch.tensor(df['score'].values)\n","for epoch in range(num_epochs):\n","    loss_per_batch = []\n","    for i in tqdm(range(0, total_len, batch_size)):\n","        X_title = titles[i:i+batch_size]\n","        X_domain = domains[i:i+batch_size]\n","        X_dayofweeks = dayofweeks[i:i+batch_size]\n","        X_hours = hours[i:i+batch_size]\n","        y_true = gt_scores[i:i+batch_size]\n","\n","        score = model(X_title, X_domain, X_dayofweeks, X_hours)\n","        \n","        optimizer.zero_grad()\n","        loss = hybrid_loss(score, y_true)\n","        loss.backward()\n","        loss_per_batch.append(loss.item())\n","        optimizer.step()\n","    \n","    print(f\"Epoch: {epoch}/{num_epochs} ; Training loss: {np.mean(loss_per_batch)}\")"]},{"cell_type":"markdown","metadata":{"_uuid":"50a50c2cdcfbfd4acd2c0d8755d9c1b17643fe46"},"source":["The model uses a linear learning rate decay to allow it to learn better once it starts converging.\n","\n","Note: in this Kaggle Notebook, the training times out after 33 epochs when committing, so I set it to 25 here. You should probably train for longer. (50+ epochs)"]},{"cell_type":"markdown","metadata":{"_uuid":"c74104921970cad33e1c2e81b19d595e8a8c1031"},"source":["## Check Predictions Against Validation Set\n","\n","Predicting against data that was not trained in the model: the model does this poorly. :("]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def prepare_for_pred(df):\n","    df['domain'] = df['url'].apply(url_to_domain)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"27cba50c64e5e9eda0e33b3530f297689cfef824","collapsed":true,"trusted":false},"outputs":[],"source":["val_size = int(split_prop * df.shape[0])\n","\n","predictions = model.predict([titles[-val_size:],\n","                             domains[-val_size:],\n","                             dayofweeks[-val_size:],\n","                             hours[-val_size:]])[:, 0]\n","\n","predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"3f9a99a8de6d8b3df499490138d3da481695b36a","collapsed":true,"trusted":false},"outputs":[],"source":["df_preds = pd.concat([pd.Series(df['title'].values[-val_size:]),\n","                      pd.Series(df['score'].values[-val_size:]),\n","                      pd.Series(predictions)],\n","                     axis=1)\n","df_preds.columns = ['title', 'actual', 'predicted']\n","# df_preds.to_csv('hn_val.csv', index=False)\n","df_preds.head(50)"]},{"cell_type":"markdown","metadata":{"_uuid":"27aa9024134e3ed086a8eac8bc1caa1009a3c979"},"source":["## Check Predictions Against Training Set\n","\n","The model should be able to predict these better."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"eefea11fef620a0f239c843ec927cc5a20278aa3","collapsed":true,"scrolled":true,"trusted":false},"outputs":[],"source":["train_size = int((1-split_prop) * df.shape[0])\n","\n","predictions = model.predict([titles[:train_size],\n","                             domains[:train_size],\n","                             dayofweeks[:train_size],\n","                             hours[:train_size]])[:, 0]\n","\n","df_preds = pd.concat([pd.Series(df['title'].values[:train_size]),\n","                      pd.Series(df['score'].values[:train_size]),\n","                      pd.Series(predictions)],\n","                     axis=1)\n","df_preds.columns = ['title', 'actual', 'predicted']\n","# df_preds.to_csv('hn_train.csv', index=False)\n","df_preds.head(50)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":1}
