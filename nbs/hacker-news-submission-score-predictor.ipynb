{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"7dc0d9be1d31ece641148d1afb09bd03dd6651e1"},"source":["# Hacker News Submission Score Predictor w/ Keras and TensorFlow\n","\n","by Max Woolf ([@minimaxir](https://minimaxir.com))"]},{"cell_type":"code","execution_count":1,"metadata":{"_uuid":"93ba18402e731c4293323a0aeb307d1109b53b0b","collapsed":true,"scrolled":true,"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import hopsworks\n","import torch\n","import torchtext\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":39,"metadata":{"_uuid":"3500e7029ecde96fc6decfb602be4a5298641eae","collapsed":true,"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>url</th>\n","      <th>score</th>\n","      <th>time</th>\n","      <th>descendants</th>\n","      <th>by</th>\n","      <th>karma</th>\n","      <th>domain</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>8535571.0</td>\n","      <td>5 Guidelines for Recovery Drills Within the AW...</td>\n","      <td>http://www.n2ws.com/blog/5-guidelines-recovery...</td>\n","      <td>2.0</td>\n","      <td>1.414700e+09</td>\n","      <td>0.0</td>\n","      <td>iamondemand</td>\n","      <td>83.0</td>\n","      <td>www.n2ws.com</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>38122977.0</td>\n","      <td>Why do we allow ourselves to hold ungrounded a...</td>\n","      <td>https://meltingasphalt.com/crony-beliefs/</td>\n","      <td>3.0</td>\n","      <td>1.698974e+09</td>\n","      <td>0.0</td>\n","      <td>TheIronYuppie</td>\n","      <td>1547.0</td>\n","      <td>meltingasphalt.com</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>7349975.0</td>\n","      <td>Japan Said to Be Ready to Impose Bitcoin Rules</td>\n","      <td>http://dealbook.nytimes.com/2014/03/05/japan-s...</td>\n","      <td>2.0</td>\n","      <td>1.394054e+09</td>\n","      <td>0.0</td>\n","      <td>JumpCrisscross</td>\n","      <td>129974.0</td>\n","      <td>dealbook.nytimes.com</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>19972463.0</td>\n","      <td>“Maslow’s pyramid” is based on an elitist misr...</td>\n","      <td>https://qz.com/work/1588491/maslow-didnt-make-...</td>\n","      <td>2.0</td>\n","      <td>1.558460e+09</td>\n","      <td>0.0</td>\n","      <td>wjSgoWPm5bWAhXB</td>\n","      <td>2456.0</td>\n","      <td>qz.com</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>21521793.0</td>\n","      <td>The Girl Who Never Came Back (1960)</td>\n","      <td>https://www.americanheritage.com/girl-who-neve...</td>\n","      <td>2.0</td>\n","      <td>1.573630e+09</td>\n","      <td>0.0</td>\n","      <td>smacktoward</td>\n","      <td>54886.0</td>\n","      <td>www.americanheritage.com</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>3471206.0</td>\n","      <td>Pasadena Cheeseburger Week - We Went There</td>\n","      <td>http://aloneinaforest.com/cheeseburger-in-para...</td>\n","      <td>1.0</td>\n","      <td>1.326731e+09</td>\n","      <td>-1.0</td>\n","      <td>darlingalice</td>\n","      <td>2.0</td>\n","      <td>aloneinaforest.com</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>3346871.0</td>\n","      <td>Recreating the original Macintosh boot beep in...</td>\n","      <td>http://romulusetrem.us/bootbeep/</td>\n","      <td>2.0</td>\n","      <td>1.323764e+09</td>\n","      <td>0.0</td>\n","      <td>pom</td>\n","      <td>73.0</td>\n","      <td>romulusetrem.us</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>17079306.0</td>\n","      <td>Google One is coming soon</td>\n","      <td>https://one.google.com</td>\n","      <td>81.0</td>\n","      <td>1.526434e+09</td>\n","      <td>86.0</td>\n","      <td>tvvocold</td>\n","      <td>1946.0</td>\n","      <td>one.google.com</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>4388709.0</td>\n","      <td>Let's Build a Tesla Museum</td>\n","      <td>http://theoatmeal.com/blog/tesla_museum</td>\n","      <td>1.0</td>\n","      <td>1.345071e+09</td>\n","      <td>-1.0</td>\n","      <td>cjdavis</td>\n","      <td>174.0</td>\n","      <td>theoatmeal.com</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>11422153.0</td>\n","      <td>Fair Source licensing is the worst thing to ha...</td>\n","      <td>http://www.techrepublic.com/article/fair-sourc...</td>\n","      <td>2.0</td>\n","      <td>1.459780e+09</td>\n","      <td>0.0</td>\n","      <td>alxsanchez</td>\n","      <td>450.0</td>\n","      <td>www.techrepublic.com</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           id                                              title  \\\n","0   8535571.0  5 Guidelines for Recovery Drills Within the AW...   \n","1  38122977.0  Why do we allow ourselves to hold ungrounded a...   \n","2   7349975.0     Japan Said to Be Ready to Impose Bitcoin Rules   \n","3  19972463.0  “Maslow’s pyramid” is based on an elitist misr...   \n","4  21521793.0                The Girl Who Never Came Back (1960)   \n","5   3471206.0         Pasadena Cheeseburger Week - We Went There   \n","6   3346871.0  Recreating the original Macintosh boot beep in...   \n","7  17079306.0                          Google One is coming soon   \n","8   4388709.0                         Let's Build a Tesla Museum   \n","9  11422153.0  Fair Source licensing is the worst thing to ha...   \n","\n","                                                 url  score          time  \\\n","0  http://www.n2ws.com/blog/5-guidelines-recovery...    2.0  1.414700e+09   \n","1          https://meltingasphalt.com/crony-beliefs/    3.0  1.698974e+09   \n","2  http://dealbook.nytimes.com/2014/03/05/japan-s...    2.0  1.394054e+09   \n","3  https://qz.com/work/1588491/maslow-didnt-make-...    2.0  1.558460e+09   \n","4  https://www.americanheritage.com/girl-who-neve...    2.0  1.573630e+09   \n","5  http://aloneinaforest.com/cheeseburger-in-para...    1.0  1.326731e+09   \n","6                   http://romulusetrem.us/bootbeep/    2.0  1.323764e+09   \n","7                             https://one.google.com   81.0  1.526434e+09   \n","8            http://theoatmeal.com/blog/tesla_museum    1.0  1.345071e+09   \n","9  http://www.techrepublic.com/article/fair-sourc...    2.0  1.459780e+09   \n","\n","   descendants               by     karma                    domain  \n","0          0.0      iamondemand      83.0              www.n2ws.com  \n","1          0.0    TheIronYuppie    1547.0        meltingasphalt.com  \n","2          0.0   JumpCrisscross  129974.0      dealbook.nytimes.com  \n","3          0.0  wjSgoWPm5bWAhXB    2456.0                    qz.com  \n","4          0.0      smacktoward   54886.0  www.americanheritage.com  \n","5         -1.0     darlingalice       2.0        aloneinaforest.com  \n","6          0.0              pom      73.0           romulusetrem.us  \n","7         86.0         tvvocold    1946.0            one.google.com  \n","8         -1.0          cjdavis     174.0            theoatmeal.com  \n","9          0.0       alxsanchez     450.0      www.techrepublic.com  "]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["# project = hopsworks.login(project='id2223_enric')\n","# fs = project.get_feature_store()\n","# hackernews_fg = fs.get_feature_group(\"hackernews_fg\", 2)\n","# query = hackernews_fg.select_all()\n","# feature_view = fs.get_or_create_feature_view(name=\"hackernews_fv\",\n","#                                   version=2,\n","#                                   description=\"Hackernews feature view\",\n","#                                   labels=[\"score\"],\n","#                                   query=query)\n","from sklearn.model_selection import train_test_split\n","from urllib.parse import urlparse\n","\n","def url_to_domain(url):\n","    parsed_url = urlparse(url)\n","\n","    domain = parsed_url.netloc\n","    return domain\n","\n","feature_view = pd.read_csv('../data/pd_combined.csv')\n","\n","train_df, test_df = train_test_split(feature_view, test_size=0.05)\n","\n","df = train_df.sample(frac=1, random_state=123).dropna().reset_index(drop=True)\n","df['domain'] = df['url'].apply(url_to_domain)\n","\n","df.head(10)"]},{"cell_type":"code","execution_count":40,"metadata":{"_uuid":"644a5e101b25edf934fe33b672ec633c1e398336","collapsed":true,"trusted":true},"outputs":[],"source":["from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import Vocab\n","\n","def pad_sequences(seq, max_len=15):\n","    seq =  torch.tensor(seq)\n","    seq = nn.ConstantPad1d((0, max_len - len(seq)), 0)(seq)\n","    return seq\n","\n","tokenizer = get_tokenizer(\"basic_english\")"]},{"cell_type":"code","execution_count":41,"metadata":{"_uuid":"b55daa311c443e0cf961a9d424b03e32baa10b9a","collapsed":true,"trusted":true},"outputs":[],"source":["from collections import Counter\n","from functools import partial\n","\n","counter = Counter()\n","for title in df['title']:\n","    counter.update(tokenizer(title))\n","vocab = Vocab(counter)\n","text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n","df['title'] = df['title'].apply(text_pipeline)\n","df['title'] = df['title'].apply(pad_sequences)"]},{"cell_type":"markdown","metadata":{"_uuid":"a924d7ba85067fa07cc35eba48c2cfe268bbcabb"},"source":["### Top Domains\n","\n","Identify the top *n* domains by count (in this case *n* = 100), then transform it to a *n*D vector for each post."]},{"cell_type":"code","execution_count":42,"metadata":{"_uuid":"bba9a0d62b8a75da61bdc0bb521507107ef5516d","collapsed":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["domain\n","github.com                4773\n","medium.com                3468\n","www.youtube.com           2783\n","www.nytimes.com           2114\n","techcrunch.com            1542\n","                          ... \n","bit.ly                     104\n","apnews.com                 103\n","blogs.wsj.com              103\n","www.usatoday.com           103\n","bits.blogs.nytimes.com     102\n","Name: count, Length: 100, dtype: int64\n"]}],"source":["num_domains = 100\n","\n","domain_counts = df['domain'].value_counts()[0:num_domains]\n","\n","print(domain_counts)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["titles = torch.stack([title for title in df['title'].values])"]},{"cell_type":"code","execution_count":47,"metadata":{"_uuid":"e8a2182d48e0af02c1db7f449ac0f8e8a7bccaad","collapsed":true,"trusted":true},"outputs":[{"data":{"text/plain":["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import LabelBinarizer\n","\n","top_domains = np.array(domain_counts.index, dtype=object)\n","\n","domain_encoder = LabelBinarizer()\n","domain_encoder.fit(top_domains)\n","\n","domains = domain_encoder.transform(df['domain'].values.astype(str))\n","domains[0]"]},{"cell_type":"markdown","metadata":{"_uuid":"892f13419e923efb918132ebd14990038753e6a0"},"source":["### Day-of-Week and Hour\n","\n","Convert day-of-week to a 7D vector and hours to a 24D vector. Both pandas and keras have useful functions for this workflow."]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["def to_categorical(y, num_classes):\n","    \"\"\" 1-hot encodes a tensor \"\"\"\n","    return np.eye(num_classes, dtype='uint8')[y]"]},{"cell_type":"code","execution_count":55,"metadata":{"_uuid":"af8e27fe36ec1e49817158de677b4070e42e3a06","collapsed":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0 0 0 1 0 0 0]\n"," [0 0 0 1 0 0 0]\n"," [0 0 0 1 0 0 0]\n"," [0 0 0 1 0 0 0]\n"," [0 0 0 1 0 0 0]]\n","[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"]}],"source":["# from keras.utils import to_categorical\n","\n","dayofweeks = to_categorical(pd.to_datetime(df['time']).dt.dayofweek, 7)\n","hours = to_categorical(pd.to_datetime(df['time']).dt.hour, 24)\n","\n","print(dayofweeks[0:5])\n","print(hours[0:5])"]},{"cell_type":"markdown","metadata":{"_uuid":"869b7d1dcc98e712eeef2b1e2b11acaee8effeb2"},"source":["## Sample Weights\n","\n","Weight `score=1` samples lower so model places a higher importance on atypical submissions."]},{"cell_type":"code","execution_count":56,"metadata":{"_uuid":"e2e59af44823777b2636f991fb4e361cbb574689","collapsed":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[1. 1. 1. 1. 1.]\n"]}],"source":["weights = np.where(df['score'].values == 1, 0.5, 1.0)\n","print(weights[0:5])"]},{"cell_type":"markdown","metadata":{"_uuid":"7ba7a108c4bf2b64bae171da853ab7ba3c512f58"},"source":["## Trend and Time on New\n","\n","Unused in final model, but kept here for reference."]},{"cell_type":"code","execution_count":57,"metadata":{"_uuid":"ff955f9b48377059ad8979ec5824afcc2c4661d4","collapsed":true,"trusted":true},"outputs":[{"data":{"text/plain":["array([[0.46796003],\n","       [0.99111293],\n","       [0.42996415],\n","       [0.73252349],\n","       [0.76044037]])"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","trend_encoder = MinMaxScaler()\n","trends = trend_encoder.fit_transform(pd.to_datetime(df['time']).values.reshape(-1, 1))\n","trends[0:5]"]},{"cell_type":"code","execution_count":58,"metadata":{"_uuid":"b2b95eb166c862d0cf3ac0c783b61dd9ceb4530b","collapsed":true,"trusted":true},"outputs":[],"source":["# newtime_encoder = MinMaxScaler()\n","# newtimes = trend_encoder.fit_transform(df['time_on_new'].values.reshape(-1, 1))\n","# newtimes[0:5]"]},{"cell_type":"markdown","metadata":{"_uuid":"4f6ab55f12a1db42e88840a5184c63317f409747"},"source":["## Build the Model Prototype"]},{"cell_type":"markdown","metadata":{"_uuid":"ec736e93b7a073e8bf68f29b80ca63dbebe3a004"},"source":["Add R^2 as a performance metric: https://jmlb.github.io/ml/2017/03/20/CoeffDetermination_CustomMetric4Keras/"]},{"cell_type":"code","execution_count":116,"metadata":{"_uuid":"6baf595ec8a40471438fcaada96b7e3f8ebf6c7a","collapsed":true,"trusted":true},"outputs":[],"source":["# from keras import backend as K\n","epsilon = 1e-7\n","def r_2(y_true, y_pred):\n","    SS_res =  torch.sum(torch.square( y_true - y_pred )) \n","    SS_tot = torch.sum(torch.square( y_true - torch.mean(y_true) ) ) \n","    return ( 1 - SS_res/(SS_tot + epsilon) )"]},{"cell_type":"markdown","metadata":{"_uuid":"ffbabccecde5331ab74518451cccc602b4dc4184"},"source":["Minimizing `mse` loss as typical for regression problems will not work, as the model will realize that selecting 1 unilaterally accomplishes this task the best.\n","\n","Instead, create a hybrid loss of `mae`, `msle`, and `poisson` (see Keras's docs for more info: https://github.com/keras-team/keras/blob/master/keras/losses.py) The latter two losses can account for very high values much better; perfect for the hyper-skewed data."]},{"cell_type":"code","execution_count":120,"metadata":{"_uuid":"bf8e5369a0124adc6d2b47748270608fc0a2bb85","collapsed":true,"trusted":true},"outputs":[],"source":["def hybrid_loss(y_true, y_pred):\n","    weight_mae = 0.1\n","    weight_msle = 1.\n","    weight_poisson = 0.1\n","    \n","    mae_loss = weight_mae * torch.mean(torch.abs(y_pred - y_true), axis=-1)\n","    \n","    first_log = torch.log(torch.clip(y_pred, 1, None) + 1.)\n","    second_log = torch.log(torch.clip(y_true, epsilon, None) + 1.)\n","    msle_loss = weight_msle * torch.mean(torch.square(first_log - second_log), axis=-1)\n","    \n","    poisson_loss = weight_poisson * torch.mean(y_pred - y_true * torch.log(y_pred + epsilon), axis=-1)\n","    return torch.mean(mae_loss + msle_loss + poisson_loss)"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]}],"source":["domains = torch.tensor(domains)\n","dayofweeks = torch.tensor(dayofweeks)\n","hours = torch.tensor(hours)"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"data":{"text/plain":["(torch.Size([129802, 15]),\n"," torch.Size([129802, 100]),\n"," torch.Size([129802, 7]),\n"," torch.Size([129802, 24]))"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["titles.shape, domains.shape, dayofweeks.shape, hours.shape"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, num_words=15, num_hidden_layers=5):\n","        super().__init__()\n","\n","        self.embedding_titles = nn.Embedding(31001, 50)\n","        self.spatial_dropout = nn.Dropout2d(0.2)\n","        self.rnn_titles = nn.LSTM(50, 128)\n","\n","        self.hidden_layers = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Linear(259, 259),\n","                nn.ReLU(),\n","                nn.BatchNorm1d(259),\n","                nn.Dropout(0.5)\n","            )\n","            for _ in range(num_hidden_layers)\n","        ])\n","\n","        self.output_layer = nn.Linear(259, 1)\n","\n","    def forward(self, input_titles, input_domains, input_dayofweeks, input_hours):\n","        embedding_titles = self.embedding_titles(input_titles)\n","        spatial_dropout = self.spatial_dropout(embedding_titles)\n","        rnn_titles, _ = self.rnn_titles(spatial_dropout.permute(1, 0, 2))\n","\n","        concat = torch.cat([rnn_titles[-1], input_domains, input_dayofweeks, input_hours], dim=1)\n","        i = 0\n","        for layer in self.hidden_layers:\n","            concat = layer(concat)\n","\n","        output = self.output_layer(concat)\n","        return output"]},{"cell_type":"code","execution_count":110,"metadata":{},"outputs":[],"source":["import torch.optim as optim\n","model = Model()\n","batch_lr = 1e-3\n","num_epochs = 100\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","lr_scheduler = optim.lr_scheduler.LinearLR(optimizer)"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/10817 [00:00<?, ?it/s]UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n","  0%|          | 4/10817 [00:00<04:33, 39.55it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:47<00:00, 37.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0/100 ; Training loss: 4.07519202375377\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:51<00:00, 37.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1/100 ; Training loss: 4.01695658219623\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:44<00:00, 37.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2/100 ; Training loss: 4.013249814462836\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:39<00:00, 38.65it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 3/100 ; Training loss: 4.011533654568707\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:31<00:00, 39.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 4/100 ; Training loss: 4.010807378475456\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:34<00:00, 39.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 5/100 ; Training loss: 4.010070092854905\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:32<00:00, 39.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 6/100 ; Training loss: 4.009645540442315\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:36<00:00, 39.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 7/100 ; Training loss: 4.009058134294455\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:35<00:00, 39.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 8/100 ; Training loss: 4.008829973905289\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:37<00:00, 38.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 9/100 ; Training loss: 4.008490686151469\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:33<00:00, 39.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 10/100 ; Training loss: 4.008407439134019\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:35<00:00, 39.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 11/100 ; Training loss: 4.008176884540046\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 10817/10817 [04:34<00:00, 39.45it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 12/100 ; Training loss: 4.008252148236976\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 222/10817 [00:05<04:16, 41.29it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[123], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m     loss_per_batch\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 21\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ; Training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(loss_per_batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from tqdm import tqdm\n","\n","batch_size = 12\n","total_len = titles.shape[0]\n","gt_scores = torch.tensor(df['score'].values)\n","for epoch in range(num_epochs):\n","    loss_per_batch = []\n","    for i in tqdm(range(0, total_len, batch_size)):\n","        X_title = titles[i:i+batch_size]\n","        X_domain = domains[i:i+batch_size]\n","        X_dayofweeks = dayofweeks[i:i+batch_size]\n","        X_hours = hours[i:i+batch_size]\n","        y_true = gt_scores[i:i+batch_size]\n","\n","        score = model(X_title, X_domain, X_dayofweeks, X_hours)\n","        \n","        optimizer.zero_grad()\n","        loss = hybrid_loss(score, y_true)\n","        loss.backward()\n","        loss_per_batch.append(loss.item())\n","        optimizer.step()\n","    \n","    print(f\"Epoch: {epoch}/{num_epochs} ; Training loss: {np.mean(loss_per_batch)}\")"]},{"cell_type":"markdown","metadata":{"_uuid":"50a50c2cdcfbfd4acd2c0d8755d9c1b17643fe46"},"source":["The model uses a linear learning rate decay to allow it to learn better once it starts converging.\n","\n","Note: in this Kaggle Notebook, the training times out after 33 epochs when committing, so I set it to 25 here. You should probably train for longer. (50+ epochs)"]},{"cell_type":"markdown","metadata":{"_uuid":"c74104921970cad33e1c2e81b19d595e8a8c1031"},"source":["## Check Predictions Against Validation Set\n","\n","Predicting against data that was not trained in the model: the model does this poorly. :("]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[],"source":["torch.save(model, 'model_v2.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import hsml\n","connection = hsml.connection()\n","mr = connection.get_model_registry()\n","ms = connection.get_model_serving()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def prepare_for_pred(title, url, time):\n","    domain = url_to_domain(url)\n","    df['domain'] = df['url'].apply(url_to_domain)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"27cba50c64e5e9eda0e33b3530f297689cfef824","collapsed":true,"trusted":false},"outputs":[],"source":["val_size = int(split_prop * df.shape[0])\n","\n","predictions = model.predict([titles[-val_size:],\n","                             domains[-val_size:],\n","                             dayofweeks[-val_size:],\n","                             hours[-val_size:]])[:, 0]\n","\n","predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"3f9a99a8de6d8b3df499490138d3da481695b36a","collapsed":true,"trusted":false},"outputs":[],"source":["df_preds = pd.concat([pd.Series(df['title'].values[-val_size:]),\n","                      pd.Series(df['score'].values[-val_size:]),\n","                      pd.Series(predictions)],\n","                     axis=1)\n","df_preds.columns = ['title', 'actual', 'predicted']\n","# df_preds.to_csv('hn_val.csv', index=False)\n","df_preds.head(50)"]},{"cell_type":"markdown","metadata":{"_uuid":"27aa9024134e3ed086a8eac8bc1caa1009a3c979"},"source":["## Check Predictions Against Training Set\n","\n","The model should be able to predict these better."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"eefea11fef620a0f239c843ec927cc5a20278aa3","collapsed":true,"scrolled":true,"trusted":false},"outputs":[],"source":["train_size = int((1-split_prop) * df.shape[0])\n","\n","predictions = model.predict([titles[:train_size],\n","                             domains[:train_size],\n","                             dayofweeks[:train_size],\n","                             hours[:train_size]])[:, 0]\n","\n","df_preds = pd.concat([pd.Series(df['title'].values[:train_size]),\n","                      pd.Series(df['score'].values[:train_size]),\n","                      pd.Series(predictions)],\n","                     axis=1)\n","df_preds.columns = ['title', 'actual', 'predicted']\n","# df_preds.to_csv('hn_train.csv', index=False)\n","df_preds.head(50)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":1}
