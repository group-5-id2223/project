{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import hopsworks\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "\n",
    "# %load_ext dotenv\n",
    "# %dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/197783\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login(project='id2223_enric')\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hackernews_fg = fs.get_feature_group(\"hackernews_fg\", 1)\n",
    "query = hackernews_fg.select_all()\n",
    "feature_view = fs.get_or_create_feature_view(name=\"hackernews_fv\",\n",
    "                                  version=1,\n",
    "                                  description=\"Hackernews feature view\",\n",
    "                                  labels=[\"score\"],\n",
    "                                  query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Reading data from Hopsworks, using ArrowFlight           \n"
     ]
    },
    {
     "ename": "FeatureStoreException",
     "evalue": "Could not read data using ArrowFlight. If the issue persists, use read_options={\"use_hive\": True} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFlightUnavailableError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py:184\u001b[0m, in \u001b[0;36mArrowFlightClient._handle_afs_exception.<locals>.decorator.<locals>.afs_error_handler_wrapper\u001b[0;34m(instance, *args, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py:209\u001b[0m, in \u001b[0;36mArrowFlightClient.read_query\u001b[0;34m(self, query_object)\u001b[0m\n\u001b[1;32m    208\u001b[0m descriptor \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mflight\u001b[38;5;241m.\u001b[39mFlightDescriptor\u001b[38;5;241m.\u001b[39mfor_command(query_encoded)\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset(descriptor)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py:201\u001b[0m, in \u001b[0;36mArrowFlightClient._get_dataset\u001b[0;34m(self, descriptor)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m, descriptor):\n\u001b[0;32m--> 201\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mget_flight_info(descriptor)\n\u001b[1;32m    202\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mdo_get(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_to_ticket(info))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyarrow/_flight.pyx:1506\u001b[0m, in \u001b[0;36mpyarrow._flight.FlightClient.get_flight_info\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyarrow/_flight.pyx:71\u001b[0m, in \u001b[0;36mpyarrow._flight.check_flight_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFlightUnavailableError\u001b[0m: Flight returned unavailable error, with message: Operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFeatureStoreException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feature_view_df \u001b[38;5;241m=\u001b[39m feature_view\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(feature_view_df[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mjoin(pd\u001b[38;5;241m.\u001b[39mDataFrame(feature_view_df[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.50\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsfs/feature_view.py:1810\u001b[0m, in \u001b[0;36mFeatureView.train_test_split\u001b[0;34m(self, test_size, train_start, train_end, test_start, test_end, description, extra_filter, statistics_config, read_options, spine)\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_train_test_split(\n\u001b[1;32m   1789\u001b[0m     test_size\u001b[38;5;241m=\u001b[39mtest_size, train_end\u001b[38;5;241m=\u001b[39mtrain_end, test_start\u001b[38;5;241m=\u001b[39mtest_start\n\u001b[1;32m   1790\u001b[0m )\n\u001b[1;32m   1791\u001b[0m td \u001b[38;5;241m=\u001b[39m training_dataset\u001b[38;5;241m.\u001b[39mTrainingDataset(\n\u001b[1;32m   1792\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1793\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1808\u001b[0m     extra_filter\u001b[38;5;241m=\u001b[39mextra_filter,\n\u001b[1;32m   1809\u001b[0m )\n\u001b[0;32m-> 1810\u001b[0m td, df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_view_engine\u001b[38;5;241m.\u001b[39mget_training_data(\n\u001b[1;32m   1811\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1812\u001b[0m     read_options,\n\u001b[1;32m   1813\u001b[0m     training_dataset_obj\u001b[38;5;241m=\u001b[39mtd,\n\u001b[1;32m   1814\u001b[0m     splits\u001b[38;5;241m=\u001b[39m[TrainingDatasetSplit\u001b[38;5;241m.\u001b[39mTRAIN, TrainingDatasetSplit\u001b[38;5;241m.\u001b[39mTEST],\n\u001b[1;32m   1815\u001b[0m     spine\u001b[38;5;241m=\u001b[39mspine,\n\u001b[1;32m   1816\u001b[0m )\n\u001b[1;32m   1817\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1818\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented version to `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(td\u001b[38;5;241m.\u001b[39mversion),\n\u001b[1;32m   1819\u001b[0m     util\u001b[38;5;241m.\u001b[39mVersionWarning,\n\u001b[1;32m   1820\u001b[0m )\n\u001b[1;32m   1821\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsfs/core/feature_view_engine.py:279\u001b[0m, in \u001b[0;36mFeatureViewEngine.get_training_data\u001b[0;34m(self, feature_view_obj, read_options, splits, training_dataset_obj, training_dataset_version, spine)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_group_accessibility(feature_view_obj)\n\u001b[1;32m    271\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch_query(\n\u001b[1;32m    272\u001b[0m         feature_view_obj,\n\u001b[1;32m    273\u001b[0m         training_dataset_version\u001b[38;5;241m=\u001b[39mtd_updated\u001b[38;5;241m.\u001b[39mversion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m         spine\u001b[38;5;241m=\u001b[39mspine,\n\u001b[1;32m    278\u001b[0m     )\n\u001b[0;32m--> 279\u001b[0m     split_df \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mget_training_data(\n\u001b[1;32m    280\u001b[0m         td_updated, feature_view_obj, query, read_options\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_training_dataset_statistics(\n\u001b[1;32m    283\u001b[0m         feature_view_obj, td_updated, split_df, calc_stat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# split df into features and labels df\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsfs/engine/python.py:546\u001b[0m, in \u001b[0;36mEngine.get_training_data\u001b[0;34m(self, training_dataset_obj, feature_view_obj, query_obj, read_options)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_training_data\u001b[39m(\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28mself\u001b[39m, training_dataset_obj, feature_view_obj, query_obj, read_options\n\u001b[1;32m    544\u001b[0m ):\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m training_dataset_obj\u001b[38;5;241m.\u001b[39msplits:\n\u001b[0;32m--> 546\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_transform_split_df(\n\u001b[1;32m    547\u001b[0m             query_obj, training_dataset_obj, feature_view_obj, read_options\n\u001b[1;32m    548\u001b[0m         )\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m         df \u001b[38;5;241m=\u001b[39m query_obj\u001b[38;5;241m.\u001b[39mread(read_options\u001b[38;5;241m=\u001b[39mread_options)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsfs/engine/python.py:596\u001b[0m, in \u001b[0;36mEngine._prepare_transform_split_df\u001b[0;34m(self, query_obj, training_dataset_obj, feature_view_obj, read_option)\u001b[0m\n\u001b[1;32m    589\u001b[0m         result_dfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_series_split(\n\u001b[1;32m    590\u001b[0m             query_obj\u001b[38;5;241m.\u001b[39mread(read_options\u001b[38;5;241m=\u001b[39mread_option),\n\u001b[1;32m    591\u001b[0m             training_dataset_obj,\n\u001b[1;32m    592\u001b[0m             event_time,\n\u001b[1;32m    593\u001b[0m         )\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    595\u001b[0m     result_dfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_split(\n\u001b[0;32m--> 596\u001b[0m         query_obj\u001b[38;5;241m.\u001b[39mread(read_options\u001b[38;5;241m=\u001b[39mread_option), training_dataset_obj\n\u001b[1;32m    597\u001b[0m     )\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# apply transformations\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# 1st parametrise transformation functions with dt split stats\u001b[39;00m\n\u001b[1;32m    601\u001b[0m transformation_function_engine\u001b[38;5;241m.\u001b[39mTransformationFunctionEngine\u001b[38;5;241m.\u001b[39mpopulate_builtin_transformation_functions(\n\u001b[1;32m    602\u001b[0m     training_dataset_obj, feature_view_obj, result_dfs\n\u001b[1;32m    603\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsfs/constructor/query.py:153\u001b[0m, in \u001b[0;36mQuery.read\u001b[0;34m(self, online, dataframe_type, read_options)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoins) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [f\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m schema]:\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas types casting only supported for feature_group.read()/query.select_all()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39msql(\n\u001b[1;32m    154\u001b[0m     sql_query,\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_store_name,\n\u001b[1;32m    156\u001b[0m     online_conn,\n\u001b[1;32m    157\u001b[0m     dataframe_type,\n\u001b[1;32m    158\u001b[0m     read_options,\n\u001b[1;32m    159\u001b[0m     schema,\n\u001b[1;32m    160\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsfs/engine/python.py:106\u001b[0m, in \u001b[0;36mEngine.sql\u001b[0;34m(self, sql_query, feature_store, online_conn, dataframe_type, read_options, schema)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     98\u001b[0m     sql_query,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m     schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    104\u001b[0m ):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m online_conn:\n\u001b[0;32m--> 106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sql_offline(\n\u001b[1;32m    107\u001b[0m             sql_query,\n\u001b[1;32m    108\u001b[0m             feature_store,\n\u001b[1;32m    109\u001b[0m             dataframe_type,\n\u001b[1;32m    110\u001b[0m             schema,\n\u001b[1;32m    111\u001b[0m             hive_config\u001b[38;5;241m=\u001b[39mread_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhive_config\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m read_options \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    112\u001b[0m         )\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdbc(\n\u001b[1;32m    115\u001b[0m             sql_query, online_conn, dataframe_type, read_options, schema\n\u001b[1;32m    116\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsfs/engine/python.py:132\u001b[0m, in \u001b[0;36mEngine._sql_offline\u001b[0;34m(self, sql_query, feature_store, dataframe_type, schema, hive_config)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sql_offline\u001b[39m(\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    125\u001b[0m     sql_query,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m     hive_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    130\u001b[0m ):\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arrow_flight_client\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mis_flyingduck_query_object(sql_query):\n\u001b[0;32m--> 132\u001b[0m         result_df \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mrun_with_loading_animation(\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading data from Hopsworks, using ArrowFlight\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    134\u001b[0m             arrow_flight_client\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mread_query,\n\u001b[1;32m    135\u001b[0m             sql_query,\n\u001b[1;32m    136\u001b[0m         )\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_hive_connection(\n\u001b[1;32m    139\u001b[0m             feature_store, hive_config\u001b[38;5;241m=\u001b[39mhive_config\n\u001b[1;32m    140\u001b[0m         ) \u001b[38;5;28;01mas\u001b[39;00m hive_conn:\n\u001b[1;32m    141\u001b[0m             \u001b[38;5;66;03m# Suppress SQLAlchemy pandas warning\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsfs/util.py:345\u001b[0m, in \u001b[0;36mrun_with_loading_animation\u001b[0;34m(message, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    346\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py:194\u001b[0m, in \u001b[0;36mArrowFlightClient._handle_afs_exception.<locals>.decorator.<locals>.afs_error_handler_wrapper\u001b[0;34m(instance, *args, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(user_message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mFeatureStoreException\u001b[0m: Could not read data using ArrowFlight. If the issue persists, use read_options={\"use_hive\": True} instead."
     ]
    }
   ],
   "source": [
    "feature_view_df = feature_view.train_test_split(test_size=0.05)\n",
    "df = pd.DataFrame(feature_view_df[0]).join(pd.DataFrame(feature_view_df[2]['score']))\n",
    "df = df.sample(frac=0.50).reset_index(drop=True)\n",
    "df = df.fillna(value=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "bins = np.linspace(df['score'].min(), df['score'].max(), num_samples + 1)\n",
    "df['score_bin'] = pd.cut(df['score'], bins=bins, labels=False, include_lowest=True)\n",
    "df_sampled = df.groupby('score_bin', group_keys=False).apply(lambda x: x.sample(1))\n",
    "df_sampled = df_sampled.drop(columns=['score_bin'])\n",
    "df = df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_processing import load_text_encoder, to_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ll1 = nn.Linear(768, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(2)\n",
    "        self.elu1 = nn.ELU()\n",
    "        self.ll2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(2)\n",
    "        self.elu2 = nn.ELU()\n",
    "        self.llf = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.elu1(self.bn1(self.ll1(x)))\n",
    "        x = self.elu2(self.bn2(self.ll2(x)))\n",
    "        x = torch.sum(x, dim=1)\n",
    "        x = self.llf(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = copy.deepcopy(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearray(arr_str):\n",
    "    arr_str = arr_str.strip(\"'\").replace('\\n', '').replace('[', '').replace(']', '').split()\n",
    "    numpy_array = np.array(arr_str, dtype=float)\n",
    "    return numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_words_from_link(link):\n",
    "    # Match alphanumeric sequences\n",
    "    url_str = \"\"\n",
    "    words = re.findall(r'\\b\\w+\\b', link)\n",
    "    remove_list = ['https', 'http', 'www']\n",
    "    final_words = [w for w in words if not(w in remove_list)]\n",
    "    for w in final_words:\n",
    "        url_str += w + \" \"\n",
    "    return url_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DfDataset(Dataset):\n",
    "#     def __init__(self, df, col):\n",
    "#         self.df = df\n",
    "#         self.col = col\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         val = self.df[self.col].iloc[idx]\n",
    "#         reg_lbl = self.df['score'].iloc[idx]\n",
    "#         if reg_lbl <= 1:\n",
    "#             cls_lbl = 0\n",
    "#             reg_lbl = reg_lbl\n",
    "#         else:\n",
    "#             cls_lbl = 1\n",
    "#             reg_lbl = reg_lbl / 2800\n",
    "#         arr = rearray(val)\n",
    "#         return arr, cls_lbl, reg_lbl\n",
    "    \n",
    "from feature_processing import to_embedding\n",
    "class DfDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df[['title', 'url', 'score']]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        title = [self.df['title'].iloc[idx]]\n",
    "        url = [extract_words_from_link(self.df['url'].iloc[idx])]\n",
    "        score = self.df['score'].iloc[idx]/280\n",
    "        \n",
    "        title_embedding = F.softmax(to_embedding(title), dim=1)\n",
    "        url_embedding = F.softmax(to_embedding(url), dim=1)\n",
    "        \n",
    "        embeddings = torch.cat([title_embedding, url_embedding], dim=0)\n",
    "        embeddings = F.softmax(embeddings, dim=0)\n",
    "        return embeddings, score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = DfDataset(train_df), DfDataset(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=12, \n",
    "                          num_workers=2, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=12,\n",
    "                        num_workers=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "optimizer = optim.AdamW(model_1.parameters(), lr=1e-5)\n",
    "lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, epochs=500, steps_per_epoch=len(train_loader))\n",
    "mse_loss = nn.MSELoss()\n",
    "bce_loss = nn.BCELoss()\n",
    "def loss_fn(output, Y):\n",
    "    cls_lbl, reg_lbl = Y[0], Y[1]\n",
    "    cls_op, reg_op = F.sigmoid(output[:, 0]), output[:, 1]\n",
    "    bce_l = bce_loss(cls_op, cls_lbl)\n",
    "    mse_l = mse_loss(reg_op * cls_lbl, reg_lbl *  cls_lbl)\n",
    "    return bce_l + mse_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4990 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/enric/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/enric/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'DfDataset' on <module '__main__' (built-in)>\n",
      "  0%|          | 0/4990 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m tr_loss_per_batch \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m val_loss_per_batch \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m      5\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m sample\n\u001b[1;32m      6\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device), Y\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:438\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:386\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1039\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1032\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1039\u001b[0m w\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(process_obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_launch(process_obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(fp\u001b[38;5;241m.\u001b[39mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    tr_loss_per_batch = []\n",
    "    val_loss_per_batch = []\n",
    "    for sample in tqdm(train_loader):\n",
    "        X, Y = sample\n",
    "        X, Y = X.to(torch.float32).to(device), Y.to(torch.float32).to(device)\n",
    "        target = model_1(X)\n",
    "        loss = mse_loss(target.squeeze(), Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss_per_batch.append(loss.item())\n",
    "        lr_scheduler.step()\n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(val_loader):\n",
    "            X, Y = sample\n",
    "            X, Y = X.to(torch.float32).to(device), Y.to(torch.float32).to(device)\n",
    "            target = model_1(X)\n",
    "            loss = mse_loss(target, Y)\n",
    "            val_loss_per_batch.append(loss.item())\n",
    "            \n",
    "    print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "    print(f\"Training loss: {np.mean(tr_loss_per_batch)} Validation Loss: {np.mean(val_loss_per_batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_1,'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = torch.load('model.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = train_loader.dataset[0]\n",
    "X_df = pd.DataFrame(X.numpy())\n",
    "input_schema = Schema(X_df)\n",
    "Y_df = pd.DataFrame([Y])\n",
    "output_schema = Schema(Y_df)\n",
    "model_schema = ModelSchema(input_schema, output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "mr = project.get_model_registry()\n",
    "hopsworks_model = mr.python.create_model(\n",
    "        name=\"hackernews_model\",\n",
    "        version=2,\n",
    "        model_schema=model_schema,\n",
    "        description=\"Hackernews upvote predictor\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8a0ff5d6da459abbb44b76ac7248d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://c.app.hopsworks.ai:443/p/197783/models/hackernews_model/2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(name: 'hackernews_model', version: 2)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hopsworks_model.save(\"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
