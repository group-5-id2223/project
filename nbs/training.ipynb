{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import hopsworks\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/197783\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login(project='id2223_enric')\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hackernews_fg = fs.get_feature_group(\"hackernews_fg\", 1)\n",
    "query = hackernews_fg.select_all()\n",
    "feature_view = fs.get_or_create_feature_view(name=\"hackernews_fv\",\n",
    "                                  version=1,\n",
    "                                  description=\"Hackernews feature view\",\n",
    "                                  labels=[\"score\"],\n",
    "                                  query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using ArrowFlight (7.77s) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: Incremented version to `8`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[              id                                              title  \\\n",
      "0        6196387  In Star Trek, does the transporter conserve th...   \n",
      "3        5258770     'Get Me a Hair Appointment and Empty My Inbox'   \n",
      "4       11296156  TripMode for Windows – Limit cellular hotspot ...   \n",
      "5        2903935                     How To: Get Out of Working Rut   \n",
      "6       20797513        Debating the Cryptographic Autonomy License   \n",
      "...          ...                                                ...   \n",
      "140035        64  Largest archive of online books about religion...   \n",
      "140036       -60                        AI Is Already Killing Books   \n",
      "140038         0                                      Book of Kells   \n",
      "140039       -32  Fortran vs Python: The counter-intuitive rise ...   \n",
      "140042        86              Alaska Airlines grounds 737 Max fleet   \n",
      "\n",
      "                                                      url          time  \\\n",
      "0               http://scifi.stackexchange.com/q/39295/63  1.376256e+09   \n",
      "3       http://online.wsj.com/article/work_and_family....  1.361469e+09   \n",
      "4       https://www.tripmode.ch/tripmode-windows-v-1-0...  1.458123e+09   \n",
      "5       http://myturnstone.com/blog/how-to-get-out-of-...  1.313771e+09   \n",
      "6       https://lwn.net/SubscriberLink/797065/de59baf9...  1.566795e+09   \n",
      "...                                                   ...           ...   \n",
      "140035                          https://sacred-texts.com/  1.704365e+09   \n",
      "140036      https://matduggan.com/ai-is-gonna-kill-books/  1.704545e+09   \n",
      "140038        https://www.worldhistory.org/Book_of_Kells/  1.704364e+09   \n",
      "140039  https://fortran-lang.discourse.group/t/the-cou...  1.704548e+09   \n",
      "140042            https://avherald.com/h?article=51354f78  1.704551e+09   \n",
      "\n",
      "        descendants               by   karma  \n",
      "0                 3          Grognak     134  \n",
      "3                 0       Brajeshwar   45062  \n",
      "4                 0             m_st     730  \n",
      "5                 0          schaapy      60  \n",
      "6                 0            Tomte  140846  \n",
      "...             ...              ...     ...  \n",
      "140035           26         Woods369      33  \n",
      "140036            8      tschumacher      59  \n",
      "140038           12  YeGoblynQueenne   20807  \n",
      "140039           32      zaikunzhang     593  \n",
      "140042            5    DarkContinent    3960  \n",
      "\n",
      "[112034 rows x 7 columns],               id                                              title  \\\n",
      "1          42559                            Hacking and Refactoring   \n",
      "2       19366052  Tunable sieving of ions using graphene oxide m...   \n",
      "7       19982915  RailsConf 2019 – Opening Keynote by David Hein...   \n",
      "10      23571550  Tactics, techniques and procedures used to tar...   \n",
      "25       3843839  WebP - A new image format for the Web (by Google)   \n",
      "...          ...                                                ...   \n",
      "140022        46  Alaska Airlines grounds Boeing 737 Max 9 plane...   \n",
      "140031       105             Chromium bug bounty money tree browser   \n",
      "140037       -87                        In-Browser Code Playgrounds   \n",
      "140040       -22                          Red Tortoise Architecture   \n",
      "140041       -64                           Live Map of Swiss Trains   \n",
      "\n",
      "                                                      url          time  \\\n",
      "1       http://www.catb.org/~esr/writings/hacking-and-...  1.187152e+09   \n",
      "2           https://www.nature.com/articles/nnano.2017.21  1.552375e+09   \n",
      "7             https://www.youtube.com/watch?v=VBwWbFpkltg  1.558543e+09   \n",
      "10      https://www.cyber.gov.au/sites/default/files/2...  1.592548e+09   \n",
      "25              https://developers.google.com/speed/webp/  1.334503e+09   \n",
      "...                                                   ...           ...   \n",
      "140022  https://www.theguardian.com/us-news/2024/jan/0...  1.704531e+09   \n",
      "140031     https://lyra.horse/misc/chromium_vrp_tree.html  1.704532e+09   \n",
      "140037    https://antonz.org/in-browser-code-playgrounds/  1.704547e+09   \n",
      "140040                      https://mattkaras.info/?p=139  1.704548e+09   \n",
      "140041                https://maps.vasile.ch/transit-sbb/  1.704552e+09   \n",
      "\n",
      "        descendants            by   karma  \n",
      "1                 0       dfranke    7138  \n",
      "2                 0    richardhod     933  \n",
      "7                 0       vvdcect     183  \n",
      "10                0       nudgeee     436  \n",
      "25               53   marcusEting     339  \n",
      "...             ...           ...     ...  \n",
      "140022          135   robin_reala   62053  \n",
      "140031           46          admp    3746  \n",
      "140037           11  todsacerdoti  130706  \n",
      "140040           13    lucidguppy    1090  \n",
      "140041           10      ano-ther    1401  \n",
      "\n",
      "[28009 rows x 7 columns],         score\n",
      "0         5.0\n",
      "3         1.0\n",
      "4         9.0\n",
      "5         1.0\n",
      "6         1.0\n",
      "...       ...\n",
      "140035   76.0\n",
      "140036   16.0\n",
      "140038   21.0\n",
      "140039   59.0\n",
      "140042   12.0\n",
      "\n",
      "[112034 rows x 1 columns],         score\n",
      "1         2.0\n",
      "2         1.0\n",
      "7         1.0\n",
      "10        1.0\n",
      "25       90.0\n",
      "...       ...\n",
      "140022  173.0\n",
      "140031  185.0\n",
      "140037   54.0\n",
      "140040   10.0\n",
      "140041   28.0\n",
      "\n",
      "[28009 rows x 1 columns]]\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1000\n",
    "bins = np.linspace(df['score'].min(), df['score'].max(), num_samples + 1)\n",
    "df['score_bin'] = pd.cut(df['score'], bins=bins, labels=False, include_lowest=True)\n",
    "df_sampled = df.groupby('score_bin', group_keys=False).apply(lambda x: x.sample(1))\n",
    "df_sampled = df_sampled.drop(columns=['score_bin'])\n",
    "df = df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_processing import load_text_encoder, to_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ll1 = nn.Linear(768, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(2)\n",
    "        self.elu1 = nn.ELU()\n",
    "        self.ll2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(2)\n",
    "        self.elu2 = nn.ELU()\n",
    "        self.llf = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.elu1(self.bn1(self.ll1(x)))\n",
    "        x = self.elu2(self.bn2(self.ll2(x)))\n",
    "        x = torch.sum(x, dim=1)\n",
    "        x = self.llf(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = copy.deepcopy(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearray(arr_str):\n",
    "    arr_str = arr_str.strip(\"'\").replace('\\n', '').replace('[', '').replace(']', '').split()\n",
    "    numpy_array = np.array(arr_str, dtype=float)\n",
    "    return numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_words_from_link(link):\n",
    "    # Match alphanumeric sequences\n",
    "    url_str = \"\"\n",
    "    words = re.findall(r'\\b\\w+\\b', link)\n",
    "    remove_list = ['https', 'http', 'www']\n",
    "    final_words = [w for w in words if not(w in remove_list)]\n",
    "    for w in final_words:\n",
    "        url_str += w + \" \"\n",
    "    return url_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DfDataset(Dataset):\n",
    "#     def __init__(self, df, col):\n",
    "#         self.df = df\n",
    "#         self.col = col\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         val = self.df[self.col].iloc[idx]\n",
    "#         reg_lbl = self.df['score'].iloc[idx]\n",
    "#         if reg_lbl <= 1:\n",
    "#             cls_lbl = 0\n",
    "#             reg_lbl = reg_lbl\n",
    "#         else:\n",
    "#             cls_lbl = 1\n",
    "#             reg_lbl = reg_lbl / 2800\n",
    "#         arr = rearray(val)\n",
    "#         return arr, cls_lbl, reg_lbl\n",
    "    \n",
    "from feature_processing import to_embedding\n",
    "class DfDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df[['title', 'url', 'score']]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        title = [self.df['title'].iloc[idx]]\n",
    "        url = [extract_words_from_link(self.df['url'].iloc[idx])]\n",
    "        score = self.df['score'].iloc[idx]/280\n",
    "        \n",
    "        title_embedding = to_embedding(title)\n",
    "        url_embedding = to_embedding(url)\n",
    "        embeddings = torch.cat([title_embedding, url_embedding], dim=0)\n",
    "        embeddings = F.softmax(embeddings, dim=0)\n",
    "        return embeddings, score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = DfDataset(train_df), DfDataset(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=12, \n",
    "                          num_workers=2, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=12,\n",
    "                        num_workers=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "optimizer = optim.AdamW(model_1.parameters(), lr=1e-5)\n",
    "lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, epochs=500, steps_per_epoch=len(train_loader))\n",
    "mse_loss = nn.MSELoss()\n",
    "bce_loss = nn.BCELoss()\n",
    "def loss_fn(output, Y):\n",
    "    cls_lbl, reg_lbl = Y[0], Y[1]\n",
    "    cls_op, reg_op = F.sigmoid(output[:, 0]), output[:, 1]\n",
    "    bce_l = bce_loss(cls_op, cls_lbl)\n",
    "    mse_l = mse_loss(reg_op * cls_lbl, reg_lbl *  cls_lbl)\n",
    "    return bce_l + mse_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:04<00:00,  2.39s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      " 67%|██████▋   | 2/3 [00:08<00:03,  3.42s/it]UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "100%|██████████| 3/3 [00:10<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500\n",
      "Training loss: 13.85665206776725 Validation Loss: 3.585315704345703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.29s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/500\n",
      "Training loss: 3.3704053119376853 Validation Loss: 3.551585594813029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.27s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/500\n",
      "Training loss: 3.0964578964092113 Validation Loss: 3.294563035170237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.30s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/500\n",
      "Training loss: 2.5227972997559442 Validation Loss: 3.147269527117411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.29s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/500\n",
      "Training loss: 2.633529824239236 Validation Loss: 3.0897798935572305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.31s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/500\n",
      "Training loss: 2.685106619640633 Validation Loss: 3.031854510307312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:03<00:00,  2.34s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/500\n",
      "Training loss: 2.749568396144443 Validation Loss: 3.2761598428090415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.33s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/500\n",
      "Training loss: 2.858276888176247 Validation Loss: 3.5787360270818076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.29s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/500\n",
      "Training loss: 3.219050963719686 Validation Loss: 3.20870574315389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.31s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/500\n",
      "Training loss: 2.908951030837165 Validation Loss: 3.87276283899943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.30s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/500\n",
      "Training loss: 2.8415291044447155 Validation Loss: 3.1664156119028726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:03<00:00,  2.37s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/500\n",
      "Training loss: 2.710576366495203 Validation Loss: 3.120285073916117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.29s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/500\n",
      "Training loss: 2.6788518406726696 Validation Loss: 3.272860328356425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.31s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/500\n",
      "Training loss: 2.6753017284252025 Validation Loss: 3.095897356669108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.33s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/500\n",
      "Training loss: 2.7082230779859753 Validation Loss: 3.1101580460866294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.30s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/500\n",
      "Training loss: 2.8006770787415682 Validation Loss: 3.1490522623062134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:03<00:00,  2.35s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/500\n",
      "Training loss: 2.6450834848262645 Validation Loss: 3.01206107934316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.31s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/500\n",
      "Training loss: 2.5891454882091947 Validation Loss: 3.1453728477160134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.30s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/500\n",
      "Training loss: 2.7218753739639565 Validation Loss: 3.011244614919027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.30s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/500\n",
      "Training loss: 2.7542148055853666 Validation Loss: 3.108887791633606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.29s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/500\n",
      "Training loss: 2.732511398968873 Validation Loss: 3.0114109913508096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.31s/it]\n",
      "100%|██████████| 3/3 [00:10<00:00,  3.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/500\n",
      "Training loss: 2.7968618913933083 Validation Loss: 3.6678649187088013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.30s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/500\n",
      "Training loss: 2.716928565943683 Validation Loss: 3.7217389742533364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:04<00:00,  2.37s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/500\n",
      "Training loss: 2.803291791015201 Validation Loss: 3.2440579334894815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.30s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/500\n",
      "Training loss: 2.779134311057903 Validation Loss: 2.998934745788574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.30s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/500\n",
      "Training loss: 2.628841604347582 Validation Loss: 3.0650745232899985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.31s/it]\n",
      "100%|██████████| 3/3 [00:10<00:00,  3.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/500\n",
      "Training loss: 2.662438538339403 Validation Loss: 3.200525999069214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.31s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28/500\n",
      "Training loss: 2.6280510635287673 Validation Loss: 3.192010283470154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.28s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/500\n",
      "Training loss: 2.7524828160250627 Validation Loss: 3.0805430809656777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.28s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/500\n",
      "Training loss: 2.574722916991622 Validation Loss: 3.0820460319519043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.30s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31/500\n",
      "Training loss: 3.104647832888144 Validation Loss: 3.169434408346812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.27s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32/500\n",
      "Training loss: 2.911080007199888 Validation Loss: 3.6468025843302407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.30s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33/500\n",
      "Training loss: 2.9830464875256575 Validation Loss: 3.1277193625768027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.28s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34/500\n",
      "Training loss: 2.743401456762243 Validation Loss: 3.1260719299316406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.32s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35/500\n",
      "Training loss: 2.807856358863689 Validation Loss: 3.216425657272339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:02<00:00,  2.30s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36/500\n",
      "Training loss: 3.1046393469527915 Validation Loss: 3.373560150464376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.29s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37/500\n",
      "Training loss: 2.626341340718446 Validation Loss: 3.0548277695973716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:00<00:00,  2.25s/it]\n",
      "100%|██████████| 3/3 [00:10<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38/500\n",
      "Training loss: 2.6495977882985717 Validation Loss: 3.2027082045873008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.28s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39/500\n",
      "Training loss: 2.874914050102234 Validation Loss: 3.085273543993632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.29s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40/500\n",
      "Training loss: 3.0121877325905695 Validation Loss: 3.1983945965766907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.28s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41/500\n",
      "Training loss: 2.667955504523383 Validation Loss: 3.138283848762512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.27s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42/500\n",
      "Training loss: 2.764554230151353 Validation Loss: 3.26604962348938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.29s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43/500\n",
      "Training loss: 2.7886421238934553 Validation Loss: 3.064253787199656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.29s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44/500\n",
      "Training loss: 2.6322112966466835 Validation Loss: 3.109840909639994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [01:01<00:00,  2.29s/it]\n",
      "100%|██████████| 3/3 [00:08<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45/500\n",
      "Training loss: 2.702819643197236 Validation Loss: 3.1918026010195413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 10/27 [00:26<00:45,  2.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m tr_loss_per_batch \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m val_loss_per_batch \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m      5\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m sample\n\u001b[1;32m      6\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device), Y\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    tr_loss_per_batch = []\n",
    "    val_loss_per_batch = []\n",
    "    for sample in tqdm(train_loader):\n",
    "        X, Y = sample\n",
    "        X, Y = X.to(torch.float32).to(device), Y.to(torch.float32).to(device)\n",
    "        target = model_1(X)\n",
    "        loss = mse_loss(target.squeeze(), Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss_per_batch.append(loss.item())\n",
    "        lr_scheduler.step()\n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(val_loader):\n",
    "            X, Y = sample\n",
    "            X, Y = X.to(torch.float32).to(device), Y.to(torch.float32).to(device)\n",
    "            target = model_1(X)\n",
    "            loss = mse_loss(target, Y)\n",
    "            val_loss_per_batch.append(loss.item())\n",
    "            \n",
    "    print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "    print(f\"Training loss: {np.mean(tr_loss_per_batch)} Validation Loss: {np.mean(val_loss_per_batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_1,'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = torch.load('model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f06d7a436d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "Exception ignored in:   File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f06d7a436d0>\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "\n",
      "AssertionError    : self._shutdown_workers()\n",
      "can only test a child process  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    }
   ],
   "source": [
    "X, Y = next(iter(train_loader))\n",
    "input_schema = Schema(X.cpu().numpy())\n",
    "output_schema = Schema(Y.cpu().numpy())\n",
    "model_schema = ModelSchema(input_schema, output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOPSWORKS_API_KEY = os.environ.get('HOPSWORKS_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/223381\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "mr = project.get_model_registry()\n",
    "hopsworks_model = mr.python.create_model(\n",
    "        name=\"hopsworks_pred\",\n",
    "        model_schema=model_schema,\n",
    "        description=\"Hopsworks upvote predictor\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a411b7aac68c4ae2a150a84bd50d2823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3a8bc17c5d43c2be669608f8f7cb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/5257745 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0a8517711a4a05bcb3fe0645aed202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/214 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://c.app.hopsworks.ai:443/p/223381/models/hopsworks_pred/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(name: 'hopsworks_pred', version: 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hopsworks_model.save(\"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
