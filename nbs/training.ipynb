{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ll1 = nn.Linear(768, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.elu1 = nn.ELU()\n",
    "        self.ll2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.elu2 = nn.ELU()\n",
    "        self.llf = nn.Linear(512, 2)\n",
    "        # self.bn3 = nn.BatchNorm1d(124),\n",
    "        # self.elu3 = nn.ELU()\n",
    "        # self.llf = nn.Linear(124, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.elu1(self.bn1(self.ll1(x)))\n",
    "        x = self.elu2(self.bn2(self.ll2(x)))\n",
    "        x = self.llf(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = nn.Sequential(\n",
    "    nn.Linear(768, 360),\n",
    "    nn.BatchNorm1d(360),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(360, 124),\n",
    "    nn.BatchNorm1d(124),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(124, 2),\n",
    "    # nn.ELU(),\n",
    "    # nn.Linear(128, 2)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = copy.deepcopy(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearray(arr_str):\n",
    "    arr_str = arr_str.strip(\"'\").replace('\\n', '').replace('[', '').replace(']', '').split()\n",
    "    numpy_array = np.array(arr_str, dtype=float)\n",
    "    return numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DfDataset(Dataset):\n",
    "    def __init__(self, df, col):\n",
    "        self.df = df\n",
    "        self.col = col\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        val = self.df[self.col].iloc[idx]\n",
    "        reg_lbl = self.df['score'].iloc[idx]\n",
    "        if reg_lbl <= 1:\n",
    "            cls_lbl = 0\n",
    "            reg_lbl = reg_lbl\n",
    "        else:\n",
    "            cls_lbl = 1\n",
    "            reg_lbl = reg_lbl / 2800\n",
    "        arr = rearray(val)\n",
    "        return arr, cls_lbl, reg_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = pd.read_csv('../data/compiled.csv')\n",
    "df_c = df_c.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = df_c[:70000], df_c[70000:]\n",
    "train_title_df = train_df[['title', 'score']]\n",
    "val_title_df = val_df[['title', 'score']]\n",
    "\n",
    "train_url_df = train_df[['url', 'score']]\n",
    "val_url_df = val_df[['url', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = DfDataset(train_title_df, col='title'), DfDataset(val_title_df, col='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=12, \n",
    "                          num_workers=2, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=12,\n",
    "                        num_workers=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "optimizer = optim.AdamW(model_1.parameters(), lr=1e-5)\n",
    "lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, epochs=500, steps_per_epoch=len(train_loader))\n",
    "mse_loss = nn.MSELoss()\n",
    "bce_loss = nn.BCELoss()\n",
    "def loss_fn(output, Y):\n",
    "    cls_lbl, reg_lbl = Y[0], Y[1]\n",
    "    cls_op, reg_op = F.sigmoid(output[:, 0]), output[:, 1]\n",
    "    bce_l = bce_loss(cls_op, cls_lbl)\n",
    "    mse_l = mse_loss(reg_op * cls_lbl, reg_lbl *  cls_lbl)\n",
    "    return bce_l + mse_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500\n",
      "Training loss: 0.652201490080189 Validation Loss: 0.6487126372915377\n",
      "Epoch: 2/500\n",
      "Training loss: 0.6419460509619894 Validation Loss: 0.6399197356878139\n",
      "Epoch: 3/500\n",
      "Training loss: 0.6388908415767155 Validation Loss: 0.6419165067964321\n",
      "Epoch: 4/500\n",
      "Training loss: 0.6360472890155382 Validation Loss: 0.6393506557321091\n",
      "Epoch: 5/500\n",
      "Training loss: 0.6338058447016401 Validation Loss: 0.6411273953297155\n",
      "Epoch: 6/500\n",
      "Training loss: 0.6318212653068717 Validation Loss: 0.6365471083840593\n",
      "Epoch: 7/500\n",
      "Training loss: 0.6300207763746826 Validation Loss: 0.6364051406975273\n",
      "Epoch: 8/500\n",
      "Training loss: 0.6272066328668169 Validation Loss: 0.6374095342785334\n",
      "Epoch: 9/500\n",
      "Training loss: 0.627186828278539 Validation Loss: 0.6356038334701273\n",
      "Epoch: 10/500\n",
      "Training loss: 0.625744749753288 Validation Loss: 0.6388505454972494\n",
      "Epoch: 11/500\n",
      "Training loss: 0.6239972193998387 Validation Loss: 0.6349751482264315\n",
      "Epoch: 12/500\n",
      "Training loss: 0.6222809226567262 Validation Loss: 0.6374529532605796\n",
      "Epoch: 13/500\n",
      "Training loss: 0.6209138607760182 Validation Loss: 0.6401854130647165\n",
      "Epoch: 14/500\n",
      "Training loss: 0.6205847581333194 Validation Loss: 0.6370257344320238\n",
      "Epoch: 15/500\n",
      "Training loss: 0.6186175040995212 Validation Loss: 0.6400331037579109\n",
      "Epoch: 16/500\n",
      "Training loss: 0.6178250607150362 Validation Loss: 0.6392342339959933\n",
      "Epoch: 17/500\n",
      "Training loss: 0.6175897602741548 Validation Loss: 0.6390494335826924\n",
      "Epoch: 18/500\n",
      "Training loss: 0.6162277348071368 Validation Loss: 0.6430866531759715\n",
      "Epoch: 19/500\n",
      "Training loss: 0.6156220870973069 Validation Loss: 0.6454873856165998\n",
      "Epoch: 20/500\n",
      "Training loss: 0.6159143259785214 Validation Loss: 0.6408515538957765\n",
      "Epoch: 21/500\n",
      "Training loss: 0.6147635904002525 Validation Loss: 0.6421077472271679\n",
      "Epoch: 22/500\n",
      "Training loss: 0.6138886266583076 Validation Loss: 0.6454857395206995\n",
      "Epoch: 23/500\n",
      "Training loss: 0.6132628948701994 Validation Loss: 0.6424252814669117\n",
      "Epoch: 24/500\n",
      "Training loss: 0.6121435784132954 Validation Loss: 0.6466135522015661\n",
      "Epoch: 25/500\n",
      "Training loss: 0.6134368038704676 Validation Loss: 0.6477606103574629\n",
      "Epoch: 26/500\n",
      "Training loss: 0.61288501507193 Validation Loss: 0.6429661296897655\n",
      "Epoch: 27/500\n",
      "Training loss: 0.6121711289183833 Validation Loss: 0.6483732014894485\n",
      "Epoch: 28/500\n",
      "Training loss: 0.6117928408834679 Validation Loss: 0.6426418276785089\n",
      "Epoch: 29/500\n",
      "Training loss: 0.6106573278758066 Validation Loss: 0.6428119320806553\n",
      "Epoch: 30/500\n",
      "Training loss: 0.6119265939770883 Validation Loss: 0.6428068343469565\n",
      "Epoch: 31/500\n",
      "Training loss: 0.6123710999882789 Validation Loss: 0.6442762314558601\n",
      "Epoch: 32/500\n",
      "Training loss: 0.6123170366036308 Validation Loss: 0.6504357823793837\n",
      "Epoch: 33/500\n",
      "Training loss: 0.6114635772806428 Validation Loss: 0.6430360595528171\n",
      "Epoch: 34/500\n",
      "Training loss: 0.6126798974515478 Validation Loss: 0.6454439433715898\n",
      "Epoch: 35/500\n",
      "Training loss: 0.6112626935516599 Validation Loss: 0.6441709146225195\n",
      "Epoch: 36/500\n",
      "Training loss: 0.6135954598796911 Validation Loss: 0.6443126492506023\n",
      "Epoch: 37/500\n",
      "Training loss: 0.6129291398826837 Validation Loss: 0.641594596939693\n",
      "Epoch: 38/500\n",
      "Training loss: 0.6140443218369414 Validation Loss: 0.6502632623810848\n",
      "Epoch: 39/500\n",
      "Training loss: 0.6159486643839749 Validation Loss: 0.6431753729744781\n",
      "Epoch: 40/500\n",
      "Training loss: 0.6148844962817658 Validation Loss: 0.6457551793872024\n",
      "Epoch: 41/500\n",
      "Training loss: 0.6158610912302319 Validation Loss: 0.644533174036504\n",
      "Epoch: 42/500\n",
      "Training loss: 0.6159070485364422 Validation Loss: 0.651016512184406\n",
      "Epoch: 43/500\n",
      "Training loss: 0.6169159668876081 Validation Loss: 0.6430136974481084\n",
      "Epoch: 44/500\n",
      "Training loss: 0.6158937992126434 Validation Loss: 0.6455452104838346\n",
      "Epoch: 45/500\n",
      "Training loss: 0.615796596251165 Validation Loss: 0.6396594224430674\n",
      "Epoch: 46/500\n",
      "Training loss: 0.6172697580367439 Validation Loss: 0.6448849422468556\n",
      "Epoch: 47/500\n",
      "Training loss: 0.618956007290712 Validation Loss: 0.6453826501786851\n",
      "Epoch: 48/500\n",
      "Training loss: 0.6191123898435138 Validation Loss: 0.6422176693054698\n",
      "Epoch: 49/500\n",
      "Training loss: 0.6195539326742573 Validation Loss: 0.6420036029043815\n",
      "Epoch: 50/500\n",
      "Training loss: 0.6209640613424071 Validation Loss: 0.6445661326177972\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(target, [Y_cls, Y_reg])\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m tr_loss_per_batch\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:193\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    189\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(inputs) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    192\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 193\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:89\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    tr_loss_per_batch = []\n",
    "    val_loss_per_batch = []\n",
    "    for sample in train_loader:\n",
    "        X, Y_cls, Y_reg = sample\n",
    "        X, Y_cls, Y_reg = X.to(torch.float32).to(device), Y_cls.to(torch.float32).to(device), Y_reg.to(torch.float32).to(device)\n",
    "        target = model_1(X)\n",
    "        loss = loss_fn(target, [Y_cls, Y_reg])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss_per_batch.append(loss.item())\n",
    "        lr_scheduler.step()\n",
    "    with torch.no_grad():\n",
    "        for sample in val_loader:\n",
    "            X, Y_cls, Y_reg = sample\n",
    "            X, Y_cls, Y_reg = X.to(torch.float32).to(device), Y_cls.to(torch.float32).to(device), Y_reg.to(torch.float32).to(device)\n",
    "            target = model_1(X)\n",
    "            loss = loss_fn(target, [Y_cls, Y_reg])\n",
    "            val_loss_per_batch.append(loss.item())\n",
    "            \n",
    "    print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "    print(f\"Training loss: {np.mean(tr_loss_per_batch)} Validation Loss: {np.mean(val_loss_per_batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
